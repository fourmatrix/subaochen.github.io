---

title: MDP学习笔记
type: post
categories:
- deeplearning
layout: post
date: 2019-06-06
tags: [deeplearning,mdp,reinforcement learning]
status: publish
published: true
use_math: true
comments: true
---

david silver大神确实功底深厚，将MDP由浅入深由表及里的讲了个透，整理笔记如下。

# 马尔科夫属性（Markov Property）

马尔科夫属性是指当前状态仅取决于上一刻的状态，和历史状态无关，即有公式：

$$
P(S_{t+1}|S_t)=P(S_{t+1}|S_1,S_2,...,S_t)
$$


进一步的，假设$$S_1,S_2,...,S_t$$相互独立：

$$
P(S_1,S_2,...,S_t)=P(S_1)P(S_2|S_1)P(S_3|S_1,S_2)...P(S_t|S_1,S_2,...,S_{t-1})\\
=P(S_1)P(S_2|S_1)P(S_3|S_2)...P(S_t|S_{t-1})
$$

# 状态转移矩阵（State Transition Matrix）

状态s到s'（在状态序列中，s'表示状态s的下一个状态）的状态转移概率表示为：

$$
P_{ss'}=P(S_{t+1}=s'|S_t=s)
$$

注意大写的S和小写的s的不同意义：大写的S表示一个随机变量，小写的s表示该随机变量当前的取值为s（或者s'）。

假定问题的状态空间$S$为$(S_1,S_2,...,S_n)$，则状态转移矩阵为：

$$
P=\left[\begin{array}{ccc}
P_{S_1S_1} & P_{S_1S_2} & ... & P_{S_1S_n}\\ 
P_{S_2S_1} & P_{S_2S_2} & ... & P_{S_2S_n}\\
... & ... & ... & ... \\
P_{S_nS_1} & P_{S_nS_2} & ... & P_{S_nS_n}

\end{array}\right]
$$

其中，$$P_{S_1S_2}$$表示状态$$S_1$$$到状态$$$S_2$$的概率。在状态转移矩阵中，每一行的概率之和为1，也很可能存在多个为0的列。

有了状态转移矩阵，那么从状态空间$S$的任意采样都可以通过状态转移矩阵计算获得该序列的概率。比如存在一个状态空间$S$的采样序列如下$$S_1S_3S_3S_5S_7$$，则有该序列的概率为：

$$
P=P(S_1S_3S_3S_5S_7)=P_{S_1}P_{S_1S_3}P_{S_3S_3}P_{S_3S_5}P_{S_5S_7}
$$


# 马尔科夫过程（Markov Process=Markov Chain ）

马尔科夫过程是一个二元组<S,P>，其中：

* S是状态空间
* P是状态转移矩阵

由S和P，我们可以对状态空间的任意采样进行概率计算，即该状态序列的达成概率。

下面是一个有趣的例子（替换了原作中的Facebook为Wechat）。假设这门课只需要三节课就可结业，那么下图表达了同学们在学习过程中的各种状态及其转移概率。比如，上第一节课的时候，你有50%的概率顺利进入了第二节课，但是也有50%的概率经受不住微信的诱惑开始不停的刷。而刷微信容易上瘾，即90%的情况下你会不停的刷，直到某个时刻（10%的概率）重新回到第一节课。在第三节课，可能有40%的概率觉得差不多了，就到Pub喝点小酒庆祝......

当然，这个例子的状态图并没有完整描述上课的所有状态序列，这只是其中的一种可能状态序列。

注意到，每个状态节点的出线概率之和一定为1。比如，第二节课有两条出线，一条指向S节点（概率为0.2），一条指向C3节点（概率为0.8）。这很容易理解：有几条出线代表了从当前状态存在几种转移到其他状态的路径。

![makov-chains-student-1](images/rl/student-mdp-orig.png)

由此，概率转移矩阵为（矩阵的列顺序为C1C2C3PubPassPSLeepWechat，如果能够标出矩阵行和列的label能够更清楚的说明问题，可惜在markdown里面不知道如何操作）：

$$
P=\left[\begin{array}{ccc}
&0.5&&&&&0.5\\
&&0.8&&&0.2&\\
&&&0.4&0.6&&\\
0.2&0.4&0.4&&&&\\
&&&&&1.0&\\
&&&&&&\\
0.1&&&&&&0.9\\

\end{array}
\right]
$$

于是，下面的采样序列都是合理的，也可以计算这样的采样序列的达成概率。

* C1C2C3PassSleep：非常认真的学生
* C1WechatWwchatC1C2Sleep：第一节课走神的学生
* C1C2C3PubC1WechatWechatWechatC1C2C3PubC3PassSleep：经常走神的学生

# Markov Reward Process（MRP）

这是强化学习中特有的结构：在Markov Process中加入reward机制，即给当前状态增加一个reward来表示**当前**这个状态到底有多好，或者有多坏，这样MPR就是一个4元组$<S,P,R,\gamma>$，其中：

* S为状态空间
* P为状态转移矩阵
* R为reward function
* $$\gamma$$为衰减因子，[0,1]，$$\gamma$$越大，则远期的reward作用越大，在return的定义中的可以看的更明显。

这里不太好理解的是R，其定义为

$$
R_s=\mathbb{E}[R_{t+1}|S_t=s]
$$

$$R_s$$表征了t时刻状态$$S_t$$的reward值。举一个例子，MRP好比行军打仗攻城拔寨，每攻下一座城池战争即进入一个新的状态（status），并且立刻获得一个奖赏（reward）。也就是说，MRP中的reward是immediately  reward，和未来的状态没有任何关系。

那么为什么其定义中使用了$$R_{t+1}$$？

期望在这里有什么用意呢？


# 马尔科夫链（Markov Chain）

马尔科夫链表示为(S,P)，其中S是事件序列，P为概率转移矩阵。

理解马尔科夫链主要是抓住如下的几点：

1. 马尔科夫链中的事件是序列事件，有先后的顺序关系，因此一般要使用条件概率。

2. 马尔科夫链中的事件符合马尔科夫属性，因此当前节点的状态只和前一个节点有关，和更早期的节点没有关系。或者说，当前节点有**足够的信息**决定下一节点的状态。

3. 马尔科夫概率转移矩阵是方阵，并且每一行的和为1。转移矩阵是马尔科夫链的完整表达，从转移矩阵可以计算获得任意采样序列的概率。

   ## 一个马尔科夫链的有趣的例子

   

   

# 马尔科夫决策过程（MDP）

