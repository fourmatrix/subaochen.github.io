{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#定义系统参数\" data-toc-modified-id=\"定义系统参数-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>定义系统参数</a></span></li><li><span><a href=\"#Q表的创建函数，初始化为0\" data-toc-modified-id=\"Q表的创建函数，初始化为0-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Q表的创建函数，初始化为0</a></span></li><li><span><a href=\"#策略\" data-toc-modified-id=\"策略-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>策略</a></span></li><li><span><a href=\"#和环境的交互\" data-toc-modified-id=\"和环境的交互-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>和环境的交互</a></span></li><li><span><a href=\"#更新环境\" data-toc-modified-id=\"更新环境-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>更新环境</a></span></li><li><span><a href=\"#游戏的实现\" data-toc-modified-id=\"游戏的实现-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>游戏的实现</a></span></li><li><span><a href=\"#执行强化学习训练\" data-toc-modified-id=\"执行强化学习训练-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>执行强化学习训练</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning是增强学习中model free的的重要算法，其基本思想是通过Q表记录并更新状态-行动的价值，使得最后获得一个“完美”的Q表：当agent处于任意状态时，查询该Q表即可获知如何行动。\n",
    "\n",
    "下面通过一个非常简单的小例子来说明Q Learning的思想（本案例主要参考了： https://morvanzhou.github.io/tutorials/ ）。这是一个来自一维世界的agent，它只能在一个固定长度的线段上左右运动，每次只能运动一格，当运动到线段的最右边时才会获得奖励：+1的reward。初始时，agent位于线段的最左边，它并不知道在线段的最右边有个“宝物”可以获得reward。\n",
    "\n",
    "下面的一篇文章可以参考：https://blog.csdn.net/Young_Gy/article/details/73485518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义系统参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 6   # the length of the 1 dimensional world\n",
    "ACTIONS = ['left', 'right']     # available actions\n",
    "EPSILON = 0.9   # greedy police，这里的意思是，即便在Q表中有对应的（最佳）Q价值，也有10%的概率随机选取action\n",
    "ALPHA = 0.1     # learning rate\n",
    "GAMMA = 0.9    # discount factor\n",
    "MAX_EPISODES = 7   # maximum episodes\n",
    "FRESH_TIME = 0.01    # fresh time for one move\n",
    "\n",
    "TERMINAL='bang' # 终止状态，当agent遇到最右边的宝物时设置此状态\n",
    "DEBUG=True # 调试时设置为True则打印更多的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q表的创建函数，初始化为0\n",
    "本案例Q表的结构如下，其中最左边的一列是状态，本案例有6个状态，即agent可以在6个格子内左右移动：\n",
    "\n",
    "|&nbsp;&nbsp;&nbsp;&nbsp;|left|right|\n",
    "|---|---|---|\n",
    "|0|0|0|\n",
    "|1|0|0|\n",
    "|2|0|0|\n",
    "|3|0|0|\n",
    "|4|0|0|\n",
    "|5|0|0|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),     # q_table initial values\n",
    "        columns=actions,    # actions's name\n",
    "    )\n",
    "    print(table)    # show table\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 策略\n",
    "这是增强学习中的策略部分，这里的策略很简单：如果平均随机采样值大于设定的epsilon或者当前状态的所有动作价值为0则随机游走探索（随机选取动作），否则从Q表选取价值最大的动作。我们的目标是不断优化Q表中的动作价值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, q_table):\n",
    "    # This is how to choose an action\n",
    "    state_actions = q_table.iloc[state, :]\n",
    "    # 如果当前状态的所有动作的价值为0，则随机选取动作\n",
    "    # 如果平均随机采样值 > EPSILON，则随机选取动作\n",
    "    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()): \n",
    "        action_name = np.random.choice(ACTIONS)\n",
    "    else:   # act greedy\n",
    "        action_name = state_actions.idxmax()\n",
    "    return action_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 和环境的交互\n",
    "环境接受agent的action并执行之，然后给出下一个状态和相应的reward。只有agent走到了最右边，环境才给予+1的reward，其他情况下reward=0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_feedback(S, A):\n",
    "    # This is how agent will interact with the environment\n",
    "    # S_: next status\n",
    "    # R: reward to action A\n",
    "    if A == 'right':    # move right\n",
    "        if S == N_STATES - 2:   # terminate\n",
    "            S_ = TERMINAL\n",
    "            R = 1\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 0\n",
    "    else:   # move left\n",
    "        R = 0\n",
    "        if S == 0:\n",
    "            S_ = S  # reach the wall\n",
    "        else:\n",
    "            S_ = S - 1\n",
    "    return S_, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新环境\n",
    "这是agent和环境交互的一部分，绘制环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_env(S, episode, step_counter):\n",
    "    # This is how environment be updated\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == TERMINAL:\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 游戏的实现\n",
    "rl = reinforcement learning\n",
    "\n",
    "这里重点区分两个概念：\n",
    "\n",
    "* q_predict，Q预测，即当前(S,A)在Q表中的值（简称Q价值），表达了在S状态下如果采取A动作的价值多少。这是在环境还没有接收并执行A动作时的Q价值，即此时A动作还没有真正执行，因此是一个预测值，或者说是上一轮（S,A）后的Q真实，如果存在上一轮的话。\n",
    "* q_target，Q真实，即（S,A）执行后的Q价值：环境接收并执行了A动作，给出了S_（下一个动作）和R(reward)，则根据Q Learning算法的更新公式可计算q_target。之所以叫做Q真实，是因为这个时候A动作已经被环境执行了，这是确凿发生的事实产生的Q价值。\n",
    "\n",
    "画个图来进一步理解：\n",
    "\n",
    "![](images/rl/q-predict-vs-q-target.png)\n",
    "\n",
    "下图说明了Q Learning的算法(see: https://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html )：\n",
    "![](images/rl/qalg.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl():\n",
    "    # main part of RL loop\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        step_counter = 0\n",
    "        S = 0\n",
    "        is_terminated = False\n",
    "        update_env(S, episode, step_counter)\n",
    "        while not is_terminated:\n",
    "            A = choose_action(S, q_table)\n",
    "            # Q表中当前(S,A)对应的值称为Q预测，即当前的(S,A)组合的价值。\n",
    "            q_predict = q_table.loc[S, A]\n",
    "            S_, R = get_env_feedback(S, A)  # take action & get next state and reward\n",
    "            if S_ != TERMINAL:\n",
    "                q_target = R + GAMMA * q_table.iloc[S_, :].max()   # next state is not terminal\n",
    "            else:\n",
    "                q_target = R     # next state is terminal\n",
    "                is_terminated = True    # terminate this episode\n",
    "\n",
    "            q_table.loc[S, A] = q_predict + ALPHA * (q_target - q_predict)  # update\n",
    "            if DEBUG == True and q_target != q_predict:\n",
    "                print(' %s episode,S(%s),A(%s),R(%.6f),S_(%s),q_p(%.6f),q_t(%.6f),q_tab[S,A](%.6f)' % (episode,S,A,R,S_,q_predict,q_target,q_table.loc[S,A]))\n",
    "                #print(q_table)\n",
    "            S = S_  # move to next state\n",
    "\n",
    "            update_env(S, episode, step_counter+1)\n",
    "            step_counter += 1\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行强化学习训练\n",
    "遗憾的是，还不知道在jupyter中如何不换行持续显示训练的过程，请高手指点。目前可以通过打开DEBUG开关观察agent的训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.0\n",
      "5   0.0    0.0\n",
      "----oT 0 episode,S(4),A(right),R(1.000000),S_(bang),q_p(0.000000),q_t(1.000000),q_tab[S,A](0.100000)\n",
      "---o-T 1 episode,S(3),A(right),R(0.000000),S_(4),q_p(0.000000),q_t(0.090000),q_tab[S,A](0.009000)\n",
      "----oT 1 episode,S(4),A(right),R(1.000000),S_(bang),q_p(0.100000),q_t(1.000000),q_tab[S,A](0.190000)\n",
      "--o--T 2 episode,S(2),A(right),R(0.000000),S_(3),q_p(0.000000),q_t(0.008100),q_tab[S,A](0.000810)\n",
      "---o-T 2 episode,S(3),A(right),R(0.000000),S_(4),q_p(0.009000),q_t(0.171000),q_tab[S,A](0.025200)\n",
      "----oT 2 episode,S(4),A(right),R(1.000000),S_(bang),q_p(0.190000),q_t(1.000000),q_tab[S,A](0.271000)\n",
      "-o---T 3 episode,S(1),A(right),R(0.000000),S_(2),q_p(0.000000),q_t(0.000729),q_tab[S,A](0.000073)\n",
      "--o--T 3 episode,S(2),A(right),R(0.000000),S_(3),q_p(0.000810),q_t(0.022680),q_tab[S,A](0.002997)\n",
      "---o-T 3 episode,S(3),A(right),R(0.000000),S_(4),q_p(0.025200),q_t(0.243900),q_tab[S,A](0.047070)\n",
      "----oT 3 episode,S(4),A(right),R(1.000000),S_(bang),q_p(0.271000),q_t(1.000000),q_tab[S,A](0.343900)\n",
      "o----T 4 episode,S(0),A(right),R(0.000000),S_(1),q_p(0.000000),q_t(0.000066),q_tab[S,A](0.000007)\n",
      "-o---T 4 episode,S(1),A(right),R(0.000000),S_(2),q_p(0.000073),q_t(0.002697),q_tab[S,A](0.000335)\n",
      "--o--T 4 episode,S(2),A(right),R(0.000000),S_(3),q_p(0.002997),q_t(0.042363),q_tab[S,A](0.006934)\n",
      "---o-T 4 episode,S(3),A(right),R(0.000000),S_(4),q_p(0.047070),q_t(0.309510),q_tab[S,A](0.073314)\n",
      "----oT 4 episode,S(4),A(right),R(1.000000),S_(bang),q_p(0.343900),q_t(1.000000),q_tab[S,A](0.409510)\n",
      "o----T 5 episode,S(0),A(right),R(0.000000),S_(1),q_p(0.000007),q_t(0.000302),q_tab[S,A](0.000036)\n",
      "-o---T 5 episode,S(1),A(right),R(0.000000),S_(2),q_p(0.000335),q_t(0.006240),q_tab[S,A](0.000926)\n",
      "--o--T 5 episode,S(2),A(right),R(0.000000),S_(3),q_p(0.006934),q_t(0.065983),q_tab[S,A](0.012839)\n",
      "---o-T 5 episode,S(3),A(right),R(0.000000),S_(4),q_p(0.073314),q_t(0.368559),q_tab[S,A](0.102839)\n",
      "----oT 5 episode,S(4),A(right),R(1.000000),S_(bang),q_p(0.409510),q_t(1.000000),q_tab[S,A](0.468559)\n",
      "o----T 6 episode,S(0),A(right),R(0.000000),S_(1),q_p(0.000036),q_t(0.000833),q_tab[S,A](0.000116)\n",
      "-o---T 6 episode,S(1),A(right),R(0.000000),S_(2),q_p(0.000926),q_t(0.011555),q_tab[S,A](0.001989)\n",
      "--o--T 6 episode,S(2),A(right),R(0.000000),S_(3),q_p(0.012839),q_t(0.092555),q_tab[S,A](0.020810)\n",
      "---o-T 6 episode,S(3),A(right),R(0.000000),S_(4),q_p(0.102839),q_t(0.421703),q_tab[S,A](0.134725)\n",
      "----oT 6 episode,S(4),A(right),R(1.000000),S_(bang),q_p(0.468559),q_t(1.000000),q_tab[S,A](0.521703)\n",
      "                                \n",
      "Q-table after training:\n",
      "\n",
      "   left     right\n",
      "0   0.0  0.000116\n",
      "1   0.0  0.001989\n",
      "2   0.0  0.020810\n",
      "3   0.0  0.134725\n",
      "4   0.0  0.521703\n",
      "5   0.0  0.000000\n"
     ]
    }
   ],
   "source": [
    "q_table = rl()\n",
    "print('\\r\\nQ-table after training:\\n')\n",
    "print(q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
