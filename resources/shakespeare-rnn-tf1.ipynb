{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RNN生成文本-shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是学习tensorflow官网资料：https://tensorflow.google.cn/tutorials/sequences/text_generation 的笔记，通过RNN喂入莎士比亚的戏剧文本，尝试让电脑自己写出莎士比亚风格的文章。运行这个简单的例子需要强大的GPU，在我的笔记本上（MX 150只有2G显存）无法运行，如果只使用CPU需要较长的时间，需要有心理准备。可以在google colab上面运行测试，速度10x以上的提升。\n",
    "\n",
    "这是一个many to many的示例。实际上，RNN可能有下图所示的几种模式(参见：http://karpathy.github.io/2015/05/21/rnn-effectiveness/)：\n",
    "![diags](images/diags.jpeg)\n",
    "\n",
    "@TODO\n",
    "\n",
    "* 加入LSTM重新测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 启用eager execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow 1.x默认没有启用eager execution，因此需要明确执行`enable_eager_execution()`打开这个开关。只有1.11以上版本才支持eager execution。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载和观察数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只要使用`tf.keras`中的方法下载的数据，默认都存放到了\\$HOME/.keras/datasets目录下。下面是我的.keras/datasets目录的内容：\n",
    "```shell\n",
    "~/.keras/datasets$ ls\n",
    "auto-mpg.data            cifar-10-batches-py.tar.gz  iris_test.csv\n",
    "cifar-100-python         fashion-mnist               iris_training.csv\n",
    "cifar-100-python.tar.gz  imdb.npz                    mnist.npz\n",
    "cifar-10-batches-py      imdb_word_index.json        shakespeare.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/subaochen/.keras/datasets/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里不使用`tf.data.Dataset.TextlineDataset`？也许是因为需要进一步对文本进行分拆处理的缘故？\n",
    "\n",
    "也没有使用`pandas`提供的方法？\n",
    "\n",
    "有机会尝试使用`Dataset`或`pandas`改写这个部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 1000 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本向量化\n",
    "文本向量化才能喂入RNN学习，需要三个步骤：\n",
    "1. 构造文本字典vocab\n",
    "1. 建立字典索引char2idx，将字典的每一个字符映射为数字\n",
    "1. 使用char2idx将文本数字化（向量化）\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> 使用tf.data.Dataset.map方法可以更方便的处理文本向量化？不过就无法观察向量化文本的过程了。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text)) # sorted保证了集合的顺序\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "# vocab是有序集合，转化为数组后其下标自然就是序号，但是不如char2idx结构直观\n",
    "# 如果模仿char2idx也很简单：idx2char = {i:u for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "text_as_int[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种方式观察一下向量化后的文本。这里没有使用matplotlib，没有太大意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'   --->    0\n",
      "' '    --->    1\n",
      "'!'    --->    2\n",
      "'$'    --->    3\n",
      "'&'    --->    4\n",
      "\"'\"    --->    5\n",
      "','    --->    6\n",
      "'-'    --->    7\n",
      "'.'    --->    8\n",
      "'3'    --->    9\n",
      "':'    --->   10\n",
      "';'    --->   11\n",
      "'?'    --->   12\n",
      "'A'    --->   13\n",
      "'B'    --->   14\n",
      "'C'    --->   15\n",
      "'D'    --->   16\n",
      "'E'    --->   17\n",
      "'F'    --->   18\n",
      "'G'    --->   19\n"
     ]
    }
   ],
   "source": [
    "# 取出char2idx前20个元素的奇怪写法。zip方法返回成对的元组，range(20)提供了序号。\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('{:6s} ---> {:4d}'.format(repr(char), char2idx[char]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(text[:13], text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造训练数据（样本数据）\n",
    "把数据喂给RNN之前，需要构造/划分好训练数据和验证数据。在这里，无需验证和测试数据，因此只需要划分好训练数据即可。下面的代码中，每次喂给RNN的训练数据是seq_length个字符。\n",
    "\n",
    "但是，实际内部处理时，RNN还是要一个一个字符消化，即RNN的输入维度是len(vocab)，参见下图(出处：http://karpathy.github.io/2015/05/21/rnn-effectiveness/ )：\n",
    "![charseq](http://softlab.sdut.edu.cn/blog/subaochen/wp-content/uploads/sites/4/2019/05/charseq.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: (), types: tf.int64>\n",
      "WARNING:tensorflow:From /home/subaochen/anaconda3/envs/tf1-cpu/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "# 每次喂入RNN的字符数。注意和后面的BATCH_SIZE的区别以及匹配\n",
    "# 为了更好的观察数据，初始的时候seq_length可以设置为10，但是执行时要恢复为100或者\n",
    "# 更大的数。当然，也可以测试不同的seq_length下的结果\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "print(char_dataset)\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "# sequences也是一个Dataset对象，但是经过了batch操作进行数据分组，每一个batch的数据\n",
    "# 长度是seq_length+1（101）.sequences用来创建输入文本和目标文本（长度为seq_length）\n",
    "# 注意：这里的batch操作和训练模型时的BATCH_SIZE没有关系，这里的batch操作纯粹\n",
    "# 为了按照指定的尺寸切分数据\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "# repl函数的意义相当于Java的toString方法\n",
    "# 注意，这里的item已经是tensor了，通过numpy()方法转化为numpy矩阵（向量）\n",
    "# numpy数组（List）的强大之处：允许接受一个list作为索引参数，因此idx2char[item.numpy()]即为根据item\n",
    "# 的数字为索引获得字符构造出一个字符串\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建输入文本和目标文本\n",
    "输入文本即参数，目标文本相当于“标签”，预测文本将和目标文本比较以计算误差。\n",
    "目标文本(target)和输入(input)文本的关系：目标文本和输入文本正好错开一个字符，即目标文本的第一个字符恰好是输入文本的第二个字符，以此类推。\n",
    "\n",
    "注意下面的代码中，dataset的shape变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] # 不包括-1即最后一个字符，总共100个字符。这就是为什么chunk的长度是101的原因\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    " \n",
    "# 注意到，sequences已经是被batch过的了，因此这里的map是针对每个batch的数据来进行的\n",
    "# 此时dataset的结果已经比较复杂了，所谓的nested structure of tensors\n",
    "# print（dateset）的结果显示其shape为：shapes: ((10,), (10,))\n",
    "# 即，dataset是一个tuple，tuple的每个数据又包含两个tuple，每个tuple是seq_length\n",
    "# 长度的向量。其中第一个tuple是input_example，第二个tuple是target_example\n",
    "dataset = sequences.map(split_input_target)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_example就是输入样本，target_example就是目标样本\n",
    "可以看出，这里的输入样本和目标样本的尺寸都是seq_length，整个文本被batch_size\n",
    "分割成了len(text_as_int)/seq_length组输入样本和目标样本\n",
    "\n",
    "训练的时候是成对喂入输入样本和目标样本的：但是，其实内部还是一个字符一个字符来计算的，即先取输入样本的第一个字符作为x和目标样本的第一个字符作为y，然后依次处理完输入样本和目标样本的每一个字符，这个batch计算完毕。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# 将take的参数设为2能看的更清楚\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练之前，先简单模拟一下预测First这个单词的过程：比如第一步（step 0），获得输入是19（F），预测值应该是47（i），以此类推。当然，这不是RNN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用批次重新构造训练数据\n",
    "\n",
    "到目前为止，使用了如下的变量来表示文本的不同形态：\n",
    "* text: 原始的文本\n",
    "* text_as_int：向量化（数字化）的字符串\n",
    "* sequences：按照seq_length+1切分的Dataset\n",
    "* dataset：将每一个seqences划分为input_text和target_text的Dataset，此时的dataset其实比sequences大了一倍\n",
    "\n",
    "到这个阶段，我们还需要将dataset中的（input_text，target_text）对进行shuffle处理。注意，这里的shuffle是以seq_length长度的input_text/target_text对为单位的，不是字符级别的shuffle。想一下dataset的数据结构。\n",
    "\n",
    "另外，还需要进一步对数据进行batch处理以便迭代训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有点奇怪的是，fit方法为什么不能通过已经定义的batch_size自动确定步长？为什么一定要通过一个steps_per_epoch参数呢？steps_per_epoch也是通过batch_size计算出来的啊，按说应该都能够达到目的的。查阅了一下2.0.0-alpha0的文档，**这个限制已经取消了**，参见：https://www.tensorflow.org/alpha/tutorials/text/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((32, 100), (32, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "# 这里的BATCH_SIZE的单位不是字符，因为此时的dataset是按照\n",
    "# ((seq_length,),(seq_length))组织的\n",
    "# 这里的32意味着，经过32次迭代，就需要遍历整个dataset，因此每次迭代需要喂入\n",
    "# 的数据尺寸如steps_per_epoch所示。\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# steps_per_epoch说明每次喂入RNN的（input_example,target_example）的个数\n",
    "# 使用model.fit时，如果传入的数据集是Dataset对象，必须显式声明steps_per_epoch参数\n",
    "# 道理很简单，否则tensorflow不知道以多大的步长循环迭代给定的Dataset。因为传入fit函数\n",
    "# 的Dataset只是经过了seq_length分组的input_text和target_text，并没有指定训练时\n",
    "# 使用多大的步长来迭代整个Dataset。\n",
    "# steps_per_epoch = len(text)//seq_length//BATCH_SIZE\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# \n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型\n",
    "\n",
    "模型分为三层：\n",
    "1. 嵌入层（layers.Embedding)。关于嵌入的概念可参考：https://tensorflow.google.cn/guide/embedding 。简单的说，嵌入层的作用是将输入(本例是输入字符的索引)映射为一个高维度向量（dense vector），其好处是可以借助于向量的方法，比如欧氏距离或者角度来度量两个向量的相似性。对于文本而言，就是两个词的相似度。\n",
    "2. GRU层（Gated Recurrent Unit）\n",
    "3. 全链接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置模型参数，实例化模型\n",
    "为了能够在笔记本电脑上运行，特意调小了embedding_dim和rnn_units两个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "# 这是输入层和输出层的维度。\n",
    "# 每一个字符都需要进行one-hot编码，因此每一个输入都是vocab_size维度的向量\n",
    "# 同样的，每一个预测的输出也是vocab_size维度的向量\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "#embedding_dim = 256\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "#rnn_units = 1024\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    # 替换rnn为LSTM\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size) # 这里不需要激活函数？softmax？\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (32, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (32, None, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 65)            33345     \n",
      "=================================================================\n",
      "Total params: 1,624,897\n",
      "Trainable params: 1,624,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先测试一下模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# input_example_batch是dataset的一个batch，这里是32个seq_length的input_text\n",
    "# 由于喂入的数据的shape是(32,seq_length)，输出example_batch_prediction的\n",
    "# shape自然就是(32,seq_length,65)\n",
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">这是为什么使用random.categorical抽取数据？</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 58,  0, 60, 46, 26, 40, 56, 28, 20, 57, 60,  0, 32, 54,  9, 55,\n",
       "        4, 46, 45, 61, 52,  2, 14, 35, 43, 45, 53, 26, 27, 25, 35, 18, 29,\n",
       "       29, 23, 12, 29, 52, 40,  2, 46, 22, 27, 64,  1,  3, 22, 28, 29, 33,\n",
       "       29, 12, 51, 29, 42, 10, 25, 62, 17, 54, 31, 48, 14, 13, 11, 43, 49,\n",
       "       47, 36,  3, 15, 49, 56, 39, 23, 20, 14, 62, 15, 49, 17, 55, 31, 19,\n",
       "       51, 59, 18, 33, 64,  4, 36, 28, 48, 62, 63,  7, 14, 30, 55])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查第0批数据？\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], \n",
    "                                        num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"\\nThree or four miles about, else had I, sir,\\nHalf an hour since brought my report.\\n\\nCOMINIUS:\\nWho's \"\n",
      "Next Char Predictions: \n",
      " ' t\\nvhNbrPHsv\\nTp3q&hgwn!BWegoNOMWFQQK?Qnb!hJOz $JPQUQ?mQd:MxEpSjBA;ekiX$CkraKHBxCkEqSGmuFUz&XPjxy-BRq'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0].numpy()])))\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义优化器和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (32, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.1745744\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恢复checkpoint\n",
    "如何检测checkoutpoint是否存在？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何解读model.summary()的输出，尤其维度的变化和参数的数量？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from checkpoint\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (32, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (32, None, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (32, None, 65)            33345     \n",
      "=================================================================\n",
      "Total params: 1,624,897\n",
      "Trainable params: 1,624,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if ckpt != None:\n",
    "  print(\"load model from checkpoint\")\n",
    "  model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=BATCH_SIZE)\n",
    "  model.load_weights(ckpt)\n",
    "  model.build(tf.TensorShape([1, None]))\n",
    "  model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subaochen/anaconda3/envs/tf1-cpu/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/348 [============================>.] - ETA: 1s - loss: 1.4848WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f37f4288cf8>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "WARNING:tensorflow:From /home/subaochen/anaconda3/envs/tf1-cpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "348/348 [==============================] - 557s 2s/step - loss: 1.4845\n",
      "Epoch 2/3\n",
      "347/348 [============================>.] - ETA: 1s - loss: 1.4244WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f37f4288cf8>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "348/348 [==============================] - 693s 2s/step - loss: 1.4242\n",
      "Epoch 3/3\n",
      "347/348 [============================>.] - ETA: 2s - loss: 1.3830WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f37f4288cf8>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "348/348 [==============================] - 712s 2s/step - loss: 1.3829\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘制训练图表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "loss=history_dict['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 产生文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 恢复到最新的checkpoint\n",
    "\n",
    "这个步骤是不是应该放在训练之前，以便积累训练的成果？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1, None, 512)            1574912   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 65)             33345     \n",
      "=================================================================\n",
      "Total params: 1,624,897\n",
      "Trainable params: 1,624,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(ckpt)\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行预测\n",
    "\n",
    "model可以接受任意长度的字符串作为参数。实际上，无论多长的字符串，model都是需要一个一个进行处理的，最终给出的是每个输入字符对应的预测字符。参考下图了解shape在各个过程的变化（出处：https://www.tensorflow.org/tutorials/sequences/text_generation）：\n",
    "![](http://softlab.sdut.edu.cn/blog/subaochen/wp-content/uploads/sites/4/2019/05/text_generation_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何观察预测结果？\n",
    "可以设置num_generate为一个**小的数字**，比如3，然后在后面的三个循环中，逐步打印出input_eval, prediction_id等的值，注意观察在不同的阶段各个向量的**维度**和**数值**的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First:\n",
      "Because your business had but yet of all tisump?\n",
      "\n",
      "Nurse:\n",
      "We\n",
      "twermard is an enemies against their royal cunquerts, decesseding,\n",
      "Now therefore suits a hoper's some father.\n",
      "\n",
      "CLAHENIS:\n",
      "The thy talk, thou dost burdent.\n",
      "\n",
      "RIVERD:\n",
      "Away then hex her son:\n",
      "The blodding know what our kinsm's worthy's old Dauch of him\n",
      "That is a loss and a wombed in the\n",
      "swetion storms BARgonet of a king:' may means!\n",
      "3 KING HENRY VI\n",
      "\n",
      "KINGINIE do could it forght him he doth choice,\n",
      "Their ears this sailt I call him the heavy brtate,\n",
      "That asce deny head us a king\n",
      "Govern'd it on a sea,\n",
      "Can covers'd his pawes in the attend, closes\n",
      "Which to the King Romee right have more but absolbate to my blood.\n",
      "\n",
      "JULIET:\n",
      "Had I redeed and may see his broaches\n",
      "What last contrants have hand of fatch his time.\n",
      "Lordly, a mirton. O cault have tongue.\n",
      "\n",
      "KING HENRY VI:\n",
      "Call him not there?\n",
      "Ah, woo, my lord, good about;'\n",
      "She wouldst have life and honest honour'd to smeet\n",
      "That remake an other fortune. I rother the crossly seak:\n",
      "As I do beseech your\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  # 构造维度合适的输入数据：[batch_size,seq_length]\n",
    "  # 这里batch_size=1，因此只需要将start_string扩展一维即可\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      # why not call model.predict()?\n",
    "      predictions = model(input_eval)\n",
    "      #print(\"predictions.shape:\",predictions.shape)\n",
    "      \n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      #print(\"predicted_id:\",predicted_id)\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n",
    "\n",
    "print(generate_text(model, start_string=u\"First:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后记\n",
    "\n",
    "从这个简单的RNN的示例中，可以学到：\n",
    "* RNN很强大，只是简单的一层LSTM（RNN）和为数不多的几次迭代训练，RNN就能够学会简单的语法结构甚至比较短的单词构成。馈入更多的训练样本和更多的迭代次数，RNN应该能够学会更多的语法特征和更多的词汇。\n",
    "* RNN的训练很消耗资源。\n",
    "* RNN能够解决很多问题，这里演示的是many to many类型的。\n",
    "* RNN并不限制输入数据的大小，也就是说，RNN的input_example可以是任意长度的。\n",
    "* 无论一次馈入多少数据，显然RNN也是一个一个进行计算和处理的；只是，在定义了`batch_size`的前提下，RNN会按照指定的`batch_size`一次汇总给出计算结果，这就是为什么输入数据要组织为`(batch_size, seq_length)`的原因：只有在输入的时候确定了`batch_size`，输出的时候才能够按照这个batch_size汇总结果为`(batch_size, vcab_size)`。\n",
    "* 使用`tf.random.categorical`获得预测结果：预测结果显然是按照batch_size给出的概率分布，可以通过`tf.random.categorical`函数方便的获得最大概率项的索引，即预测值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进一步的练习目标\n",
    "* 切分数据集为train_set, cross_validation_set, test_set，并监控loss, val_loss, test_loss的变化\n",
    "* 启用earlyStopping\n",
    "* 换其他数据集测试一下，比如全唐诗？全宋词？\n",
    "* 将字典单位换位“单词”，即不是以字符为单位，而是以单词为单位切分原始文本\n",
    "* 重新组织程序，更加模块化，使得持续训练和测试更加方便\n",
    "* 将这里学到的技术迁移到many to one的应用场景，比如金融领域的股价走势预测；以及one to many的应用场景，比如？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
