{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RNN生成文本-shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是学习tensorflow官网资料：https://tensorflow.google.cn/tutorials/sequences/text_generation 的笔记，通过RNN喂入莎士比亚的戏剧文本，尝试让电脑自己写出莎士比亚风格的文章。运行这个简单的例子需要强大的GPU，在我的笔记本上（MX 150只有2G显存）无法运行，如果只使用CPU需要较长的时间，需要有心理准备。可以在google colab上面运行测试，速度10x以上的提升。\n",
    "\n",
    "这是一个many to many的示例。实际上，RNN可能有下图所示的几种模式(参见：http://karpathy.github.io/2015/05/21/rnn-effectiveness/)：\n",
    "![diags](images/diags.jpeg)\n",
    "\n",
    "@TODO\n",
    "\n",
    "* 加入LSTM重新测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 启用eager execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow 1.x默认没有启用eager execution，因此需要明确执行`enable_eager_execution()`打开这个开关。只有1.11以上版本才支持eager execution。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载和观察数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只要使用`tf.keras`中的方法下载的数据，默认都存放到了\\$HOME/.keras/datasets目录下。下面是我的.keras/datasets目录的内容：\n",
    "```shell\n",
    "~/.keras/datasets$ ls\n",
    "auto-mpg.data            cifar-10-batches-py.tar.gz  iris_test.csv\n",
    "cifar-100-python         fashion-mnist               iris_training.csv\n",
    "cifar-100-python.tar.gz  imdb.npz                    mnist.npz\n",
    "cifar-10-batches-py      imdb_word_index.json        shakespeare.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/subaochen/.keras/datasets/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(path_to_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里不使用`tf.data.Dataset.TextlineDataset`？也许是因为需要进一步对文本进行分拆处理的缘故？\n",
    "\n",
    "也没有使用`pandas`提供的方法？\n",
    "\n",
    "有机会尝试使用`Dataset`或`pandas`改写这个部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 1000 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本向量化\n",
    "文本向量化才能喂入RNN学习，需要三个步骤：\n",
    "1. 构造文本字典vocab\n",
    "1. 建立字典索引char2idx，将字典的每一个字符映射为数字\n",
    "1. 使用char2idx将文本数字化（向量化）\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> 使用tf.data.Dataset.map方法可以更方便的处理文本向量化？不过就无法观察向量化文本的过程了。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text)) # sorted保证了集合的顺序\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "# vocab是有序集合，转化为数组后其下标自然就是序号，但是不如char2idx结构直观\n",
    "# 如果模仿char2idx也很简单：idx2char = {i:u for i,u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "text_as_int[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种方式观察一下向量化后的文本。这里没有使用matplotlib，没有太大意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'   --->    0\n",
      "' '    --->    1\n",
      "'!'    --->    2\n",
      "'$'    --->    3\n",
      "'&'    --->    4\n",
      "\"'\"    --->    5\n",
      "','    --->    6\n",
      "'-'    --->    7\n",
      "'.'    --->    8\n",
      "'3'    --->    9\n",
      "':'    --->   10\n",
      "';'    --->   11\n",
      "'?'    --->   12\n",
      "'A'    --->   13\n",
      "'B'    --->   14\n",
      "'C'    --->   15\n",
      "'D'    --->   16\n",
      "'E'    --->   17\n",
      "'F'    --->   18\n",
      "'G'    --->   19\n"
     ]
    }
   ],
   "source": [
    "# 取出char2idx前20个元素的奇怪写法。zip方法返回成对的元组，range(20)提供了序号。\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('{:6s} ---> {:4d}'.format(repr(char), char2idx[char]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(text[:13], text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造训练数据（样本数据）\n",
    "把数据喂给RNN之前，需要构造/划分好训练数据和验证数据。在这里，无需验证和测试数据，因此只需要划分好训练数据即可。下面的代码中，每次喂给RNN的训练数据是seq_length个字符。\n",
    "\n",
    "但是，实际内部处理时，RNN还是要一个一个字符消化，即RNN的输入维度是len(vocab)，参见下图(出处：http://karpathy.github.io/2015/05/21/rnn-effectiveness/ )：\n",
    "![charseq](http://softlab.sdut.edu.cn/blog/subaochen/wp-content/uploads/sites/4/2019/05/charseq.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: (), types: tf.int64>\n",
      "WARNING:tensorflow:From /home/subaochen/anaconda3/envs/tf1-cpu/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "# 每次喂入RNN的字符数。注意和后面的BATCH_SIZE的区别以及匹配\n",
    "# 为了更好的观察数据，初始的时候seq_length可以设置为10，但是执行时要恢复为100或者\n",
    "# 更大的数。当然，也可以测试不同的seq_length下的结果\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "print(char_dataset)\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "# sequences也是一个Dataset对象，但是经过了batch操作进行数据分组，每一个batch的数据\n",
    "# 长度是seq_length+1（101）.sequences用来创建输入文本和目标文本（长度为seq_length）\n",
    "# 注意：这里的batch操作和训练模型时的BATCH_SIZE没有关系，这里的batch操作纯粹\n",
    "# 为了按照指定的尺寸切分数据\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "# repl函数的意义相当于Java的toString方法\n",
    "# 注意，这里的item已经是tensor了，通过numpy()方法转化为numpy矩阵（向量）\n",
    "# numpy数组（List）的强大之处：允许接受一个list作为索引参数，因此idx2char[item.numpy()]即为根据item\n",
    "# 的数字为索引获得字符构造出一个字符串\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建输入文本和目标文本\n",
    "输入文本即参数，目标文本相当于“标签”，预测文本将和目标文本比较以计算误差。\n",
    "目标文本(target)和输入(input)文本的关系：目标文本和输入文本正好错开一个字符，即目标文本的第一个字符恰好是输入文本的第二个字符，以此类推。\n",
    "\n",
    "注意下面的代码中，dataset的shape变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1] # 不包括-1即最后一个字符，总共100个字符。这就是为什么chunk的长度是101的原因\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    " \n",
    "# 注意到，sequences已经是被batch过的了，因此这里的map是针对每个batch的数据来进行的\n",
    "# 此时dataset的结果已经比较复杂了，所谓的nested structure of tensors\n",
    "# print（dateset）的结果显示其shape为：shapes: ((10,), (10,))\n",
    "# 即，dataset是一个tuple，tuple的每个数据又包含两个tuple，每个tuple是seq_length\n",
    "# 长度的向量。其中第一个tuple是input_example，第二个tuple是target_example\n",
    "dataset = sequences.map(split_input_target)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_example就是输入样本，target_example就是目标样本\n",
    "可以看出，这里的输入样本和目标样本的尺寸都是seq_length，整个文本被batch_size\n",
    "分割成了len(text_as_int)/seq_length组输入样本和目标样本\n",
    "\n",
    "训练的时候是成对喂入输入样本和目标样本的：但是，其实内部还是一个字符一个字符来计算的，即先取输入样本的第一个字符作为x和目标样本的第一个字符作为y，然后依次处理完输入样本和目标样本的每一个字符，这个batch计算完毕。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# 将take的参数设为2能看的更清楚\n",
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练之前，先简单模拟一下预测First这个单词的过程：比如第一步（step 0），获得输入是19（F），预测值应该是47（i），以此类推。当然，这不是RNN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用批次重新构造训练数据\n",
    "\n",
    "到目前为止，使用了如下的变量来表示文本的不同形态：\n",
    "* text: 原始的文本\n",
    "* text_as_int：向量化（数字化）的字符串\n",
    "* sequences：按照seq_length+1切分的Dataset\n",
    "* dataset：将每一个seqences划分为input_text和target_text的Dataset，此时的dataset其实比sequences大了一倍\n",
    "\n",
    "到这个阶段，我们还需要将dataset中的（input_text，target_text）对进行shuffle处理。注意，这里的shuffle是以seq_length长度的input_text/target_text对为单位的，不是字符级别的shuffle。想一下dataset的数据结构。\n",
    "\n",
    "另外，还需要进一步对数据进行batch处理以便迭代训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有点奇怪的是，fit方法为什么不能通过已经定义的batch_size自动确定步长？为什么一定要通过一个steps_per_epoch参数呢？steps_per_epoch也是通过batch_size计算出来的啊，按说应该都能够达到目的的。查阅了一下2.0.0-alpha0的文档，**这个限制已经取消了**，参见：https://www.tensorflow.org/alpha/tutorials/text/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((32, 100), (32, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "# 这里的BATCH_SIZE的单位不是字符，因为此时的dataset是按照\n",
    "# ((seq_length,),(seq_length))组织的\n",
    "# 这里的32意味着，经过32次迭代，就需要遍历整个dataset，因此每次迭代需要喂入\n",
    "# 的数据尺寸如steps_per_epoch所示。\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# steps_per_epoch说明每次喂入RNN的（input_example,target_example）的个数\n",
    "# 使用model.fit时，如果传入的数据集是Dataset对象，必须显式声明steps_per_epoch参数\n",
    "# 道理很简单，否则tensorflow不知道以多大的步长循环迭代给定的Dataset。因为传入fit函数\n",
    "# 的Dataset只是经过了seq_length分组的input_text和target_text，并没有指定训练时\n",
    "# 使用多大的步长来迭代整个Dataset。\n",
    "# steps_per_epoch = len(text)//seq_length//BATCH_SIZE\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# \n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型\n",
    "\n",
    "模型分为三层：\n",
    "1. 嵌入层（layers.Embedding)。关于嵌入的概念可参考：https://tensorflow.google.cn/guide/embedding 。简单的说，嵌入层的作用是将输入(本例是输入字符的索引)映射为一个高维度向量（dense vector），其好处是可以借助于向量的方法，比如欧氏距离或者角度来度量两个向量的相似性。对于文本而言，就是两个词的相似度。\n",
    "2. GRU层（Gated Recurrent Unit）\n",
    "3. 全链接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置模型参数，实例化模型\n",
    "为了能够在笔记本电脑上运行，特意调小了embedding_dim和rnn_units两个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "# 这是输入层和输出层的维度。\n",
    "# 每一个字符都需要进行one-hot编码，因此每一个输入都是vocab_size维度的向量\n",
    "# 同样的，每一个预测的输出也是vocab_size维度的向量\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "#embedding_dim = 256\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "#rnn_units = 1024\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    # 替换rnn为LSTM\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size) # 这里不需要激活函数？softmax？\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (32, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (32, None, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 65)            33345     \n",
      "=================================================================\n",
      "Total params: 1,624,897\n",
      "Trainable params: 1,624,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先测试一下模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# input_example_batch是dataset的一个batch，这里是32个seq_length的input_text\n",
    "# 由于喂入的数据的shape是(32,seq_length)，输出example_batch_prediction的\n",
    "# shape自然就是(32,seq_length,65)\n",
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">这是为什么使用random.categorical抽取数据？</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 64, 63, 27, 62,  3,  6, 19, 48, 57, 26, 27, 36, 21, 15,  9, 18,\n",
       "       22,  7, 44, 11, 35, 37, 26, 11, 47, 15, 15, 31, 37, 36, 61, 25,  5,\n",
       "       23, 61, 26, 24, 64, 39, 12, 20, 13, 58, 54, 31, 32,  0, 57, 39,  5,\n",
       "       53, 15, 62, 43, 43, 51,  0, 63,  0,  6, 40,  0, 57, 37, 59,  9, 62,\n",
       "       42, 22, 61, 28, 23, 10, 36, 38,  5, 29, 13, 37, 47, 27, 42, 63, 13,\n",
       "       12, 54, 53, 62, 36, 58, 43, 24, 56, 37, 23, 47, 47, 22,  3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查第0批数据？\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], \n",
    "                                        num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'grave elders, to desire\\nThe present consul, and last general\\nIn our well-found successes, to report\\n'\n",
      "Next Char Predictions: \n",
      " \"!zyOx$,GjsNOXIC3FJ-f;WYN;iCCSYXwM'KwNLza?HAtpST\\nsa'oCxeem\\ny\\n,b\\nsYu3xdJwPK:XZ'QAYiOdyA?poxXteLrYKiiJ$\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0].numpy()])))\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义优化器和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (32, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.1742725\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恢复checkpoint\n",
    "如何检测checkoutpoint是否存在？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if ckpt != None:\n",
    "  print(\"load model from checkpoint\")\n",
    "  model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=BATCH_SIZE)\n",
    "  model.load_weights(ckpt)\n",
    "  model.build(tf.TensorShape([1, None]))\n",
    "  model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subaochen/anaconda3/envs/tf1-cpu/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/348 [============================>.] - ETA: 1s - loss: 2.3692WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f79a03ee828>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "WARNING:tensorflow:From /home/subaochen/anaconda3/envs/tf1-cpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "348/348 [==============================] - 574s 2s/step - loss: 2.3678\n",
      "Epoch 2/3\n",
      "347/348 [============================>.] - ETA: 1s - loss: 1.7566WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f79a03ee828>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "348/348 [==============================] - 563s 2s/step - loss: 1.7563\n",
      "Epoch 3/3\n",
      "347/348 [============================>.] - ETA: 1s - loss: 1.5764WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f79a03ee828>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "348/348 [==============================] - 560s 2s/step - loss: 1.5760\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘制训练图表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "loss=history_dict['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 产生文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 恢复到最新的checkpoint\n",
    "\n",
    "这个步骤是不是应该放在训练之前，以便积累训练的成果？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 512)            1574912   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             33345     \n",
      "=================================================================\n",
      "Total params: 1,624,897\n",
      "Trainable params: 1,624,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(ckpt)\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行预测\n",
    "\n",
    "model可以接受任意长度的字符串作为参数。实际上，无论多长的字符串，model都是需要一个一个进行处理的，最终给出的是每个输入字符对应的预测字符。参考下图了解shape在各个过程的变化（出处：https://www.tensorflow.org/tutorials/sequences/text_generation）：\n",
    "![](http://softlab.sdut.edu.cn/blog/subaochen/wp-content/uploads/sites/4/2019/05/text_generation_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何观察预测结果？\n",
    "可以设置num_generate为一个**小的数字**，比如3，然后在后面的三个循环中，逐步打印出input_eval, prediction_id等的值，注意观察在不同的阶段各个向量的**维度**和**数值**的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First:\n",
      "Set me in my life, shill saved we cannot pirture houses,\n",
      "From Lord unnour forth mine readfround placmif:\n",
      "She stratue here again; desembred, so is ntwire gronation.\n",
      "The lave,\n",
      "Ther away the prayers.\n",
      "\n",
      "MENENIUS:\n",
      "Now, hou werpho thy plafe, made as honest the comes:\n",
      "Becond eemill. Reventant and drave us.\n",
      "How not thou art of Namicient detraced prevends,\n",
      "Upon he gids trual by a man farewence to due,\n",
      "Or, look tho amfition Tuurssly should in:\n",
      "Our worthing reture in the fault him myself\n",
      "As commonian dried, command slamen-w\n",
      "Now he\n",
      "wor good our base sol, nither command some forsuar: he has doy wit\n",
      "such grong, his comfort o' thair prest, brave\n",
      "The ablickle dreigns.\n",
      "\n",
      "MORCALA: I love this dead, and fallow\n",
      "For Chimaifor petchantom gentles wild ready,\n",
      "that if through beack in none revongering friends,\n",
      "Ceftrence is a bleasure of life, trange, drance'd\n",
      "The grave dain stirst of the sunks te and Gap's Hancy'd dead,\n",
      "Befuir is than we preaso, nos. What, wen it it\n",
      "Of thing twong of your reportity,\n",
      "Alassows up\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  # 构造维度合适的输入数据：[batch_size,seq_length]\n",
    "  # 这里batch_size=1，因此只需要将start_string扩展一维即可\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      # why not call model.predict()?\n",
    "      predictions = model(input_eval)\n",
    "      #print(\"predictions.shape:\",predictions.shape)\n",
    "      \n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      #print(\"predicted_id:\",predicted_id)\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n",
    "\n",
    "print(generate_text(model, start_string=u\"First:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后记\n",
    "\n",
    "从这个简单的RNN的示例中，可以学到：\n",
    "* RNN很强大，只是简单的一层LSTM（RNN）和为数不多的几次迭代训练，RNN就能够学会简单的语法结构甚至比较短的单词构成。馈入更多的训练样本和更多的迭代次数，RNN应该能够学会更多的语法特征和更多的词汇。\n",
    "* RNN的训练很消耗资源。\n",
    "* RNN能够解决很多问题，这里演示的是many to many类型的。\n",
    "* RNN并不限制输入数据的大小，也就是说，RNN的input_example可以是任意长度的。\n",
    "* 无论一次馈入多少数据，显然RNN也是一个一个进行计算和处理的；只是，在定义了`batch_size`的前提下，RNN会按照指定的`batch_size`一次汇总给出计算结果，这就是为什么输入数据要组织为`(batch_size, seq_length)`的原因：只有在输入的时候确定了`batch_size`，输出的时候才能够按照这个batch_size汇总结果为`(batch_size, vcab_size)`。\n",
    "* 使用`tf.random.categorical`获得预测结果：预测结果显然是按照batch_size给出的概率分布，可以通过`tf.random.categorical`函数方便的获得最大概率项的索引，即预测值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进一步的练习目标\n",
    "* 切分数据集为train_set, cross_validation_set, test_set，并监控loss, val_loss, test_loss的变化\n",
    "* 启用earlyStopping\n",
    "* 换其他数据集测试一下，比如全唐诗？全宋词？\n",
    "* 将字典单位换位“单词”，即不是以字符为单位，而是以单词为单位切分原始文本\n",
    "* 重新组织程序，更加模块化，使得持续训练和测试更加方便\n",
    "* 将这里学到的技术迁移到many to one的应用场景，比如金融领域的股价走势预测；以及one to many的应用场景，比如？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
