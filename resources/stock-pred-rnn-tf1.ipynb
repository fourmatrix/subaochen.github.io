{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#最近N天的走势放大图\" data-toc-modified-id=\"最近N天的走势放大图-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>最近N天的走势放大图</a></span></li><li><span><a href=\"#最近N天的走势预测\" data-toc-modified-id=\"最近N天的走势预测-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>最近N天的走势预测</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RNN预测股票下一日的收盘价\n",
    "\n",
    "这基本是一个不可能完成的任务，不过作为RNN的练习，还是一个不错的题目：有数据，有场景，有吸引力。本案例主要参考：https://github.com/DarkKnight1991/Stock-Price-Prediction ，这是一个`many-to-one`的RNN案例，即通过前60日股价数据（open,close,high,low,volume）预测下一日的收盘价，其中feature size=5。\n",
    "\n",
    "先上运行结果图：![prediction result](https://raw.githubusercontent.com/subaochen/subaochen.github.io/master/images/stock-pred-result.png)\n",
    "\n",
    "But，这里仅仅是一个练习，不作为投资参考，请参见：https://www.blueskycapitalmanagement.com/machine-learning-in-finance-why-you-should-not-use-lstms-to-predict-the-stock-market/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入必要的包\n",
    "这里使用了sklearn包的MinMaxScaler进行数据的预处理，主要是归一化处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import pickle\n",
    "#from keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "#from keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras import optimizers\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "os.environ['TZ'] = 'Asia/Shanghai'  # to set timezone; needed when running on cloud\n",
    "time.tzset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集中设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 20,  # 20<16<10, 25 was a bust\n",
    "    \"epochs\": 30, # 由于启用了earlyStopping机制，通常会提前终止\n",
    "    \"lr\": 0.00010000, # 学习率\n",
    "    \"time_steps\": 60, # RNN的(最大？)滑动窗口大小，这是使用60日的数据预测下一日的某个特征\n",
    "    \"feature_size\": 5 # 5个特征\n",
    "}\n",
    "\n",
    "iter_changes = \"dropout_layers_0.4_0.4\"\n",
    "DATA_FILE=\"ge.us.txt\"\n",
    "PATH_TO_DRIVE_ML_DATA=\"./\"\n",
    "INPUT_PATH = PATH_TO_DRIVE_ML_DATA+\"inputs\"\n",
    "OUTPUT_PATH = PATH_TO_DRIVE_ML_DATA+\"outputs/\"+time.strftime(\"%Y-%m-%d\")+\"/\"+iter_changes\n",
    "TIME_STEPS = params[\"time_steps\"]\n",
    "FEATURE_SIZE = params[\"feature_size\"]\n",
    "BATCH_SIZE = params[\"batch_size\"]\n",
    "stime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory recreated ./outputs/2019-07-21/dropout_layers_0.4_0.4\n"
     ]
    }
   ],
   "source": [
    "# check if directory already exists\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    print(\"Directory created\", OUTPUT_PATH)\n",
    "else:\n",
    "    os.rename(OUTPUT_PATH, OUTPUT_PATH+str(stime))\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    print(\"Directory recreated\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造训练数据的方法参见下图，注意颜色相同的矩形块框出了输入数据和预测数据，图中的窗口尺寸（time_steps）是3，即使用前3天的数据预测下一日的收盘价。显然，如果样本数为`N`，则可划分的输入样本数为`N-time_steps`。\n",
    "\n",
    "![build serial time data](https://raw.githubusercontent.com/subaochen/subaochen.github.io/master/images/stock-pred-data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ge.us.txt']\n"
     ]
    }
   ],
   "source": [
    "def print_time(text, stime):\n",
    "    seconds = (time.time()-stime)\n",
    "    print(text, seconds//60,\"minutes : \",np.round(seconds%60),\"seconds\")\n",
    "\n",
    "\n",
    "def trim_dataset(mat,batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if no_of_rows_drop > 0:\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat\n",
    "\n",
    "\n",
    "def build_timeseries(mat, y_col_index):\n",
    "    \"\"\"\n",
    "    Converts ndarray into timeseries format and supervised data format. Takes first TIME_STEPS\n",
    "    number of rows as input and sets the TIME_STEPS+1th data as corresponding output and so on.\n",
    "    :param mat: ndarray which holds the dataset，shape：\n",
    "    :param y_col_index: index of column which acts as output\n",
    "    :return: returns two ndarrays-- input and output in format suitable to feed\n",
    "    to LSTM.\n",
    "    \"\"\"\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    # 先构造空的x，y\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    print(\"dim_0\",dim_0)\n",
    "    for i in tqdm_notebook(range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i] # [i,TIME_STEPS+i)，x不包含TIME_STEPS+i这一行\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index] # TIME_STEPS行x对应一个y。如果存在多个预测，修改这里\n",
    "#         if i < 10:\n",
    "#           print(i,\"-->\", x[i,-1,:], y[i])\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "stime = time.time()\n",
    "print(os.listdir(INPUT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造训练数据\n",
    "\n",
    "Again，RNN的输入数据要求的shape是(batch_size, time_steps, feature_size)\n",
    "\n",
    "数据归一化处理的公式：$$X_{scaled}=\\frac{X-X_{min}(axis=0)}{X_{max}(axis=0)-X_{min}(axis=0)}\\times(max-min)+min$$\n",
    "\n",
    "使用MinMaxScaler进行归一化处理时要注意，在训练数据集和测试数据集上要执行同样的缩放标准，因此fit只能执行一次，或者只能执行一次fit_transform函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14058, 7)\n",
      "             Date   Open    High    Low  Close     Volume  OpenInt\n",
      "14053  2017-11-06  20.52  20.530  20.08  20.13   60641787        0\n",
      "14054  2017-11-07  20.17  20.250  20.12  20.21   41622851        0\n",
      "14055  2017-11-08  20.21  20.320  20.07  20.12   39672190        0\n",
      "14056  2017-11-09  20.04  20.071  19.85  19.99   50831779        0\n",
      "14057  2017-11-10  19.98  20.680  19.90  20.49  100698474        0\n",
      "Date        object\n",
      "Open       float64\n",
      "High       float64\n",
      "Low        float64\n",
      "Close      float64\n",
      "Volume       int64\n",
      "OpenInt      int64\n",
      "dtype: object\n",
      "Train--Test size 11246 2812\n",
      "Deleting unused dataframes of total size(KB) 3267\n",
      "Are any NaNs present in train/test matrices? False False\n",
      "dim_0 11186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a52103046614120a07b58543aef1009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11186), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (11186, 60, 5) (11186,)\n",
      "Batch trimmed x_t size (11180, 60, 5)\n",
      "Batch trimmed y_t size (11180,)\n"
     ]
    }
   ],
   "source": [
    "df_ge = pd.read_csv(os.path.join(INPUT_PATH, DATA_FILE))\n",
    "print(df_ge.shape)\n",
    "print(df_ge.tail())\n",
    "tqdm_notebook.pandas('Processing...')\n",
    "print(df_ge.dtypes)\n",
    "# 训练需要的数据字段列表，去掉了日期和OpenInt\n",
    "train_cols = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n",
    "df_train, df_test = train_test_split(df_ge, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "print(\"Train--Test size\", len(df_train), len(df_test))\n",
    "\n",
    "# scale the feature MinMax, build array\n",
    "# 只需要感兴趣的字段\n",
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "# 几个方法的区别，参见：https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "# fit，计算给定集合的最大值和最小值，以便归一化计算\n",
    "# transform，在fit的基础上对数据进行归一化处理\n",
    "# fit_transform，将fit和transform结合起来，在一个函数完成\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "# 注意这里不能再次fit，在训练和测试数据集上要执行同样的缩放标准\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols].values)\n",
    "\n",
    "print(\"Deleting unused dataframes of total size(KB)\",\n",
    "      (sys.getsizeof(df_ge)+sys.getsizeof(df_train)+sys.getsizeof(df_test))//1024)\n",
    "\n",
    "del df_ge\n",
    "del df_test\n",
    "del df_train\n",
    "del x\n",
    "\n",
    "print(\"Are any NaNs present in train/test matrices?\",np.isnan(x_train).any(), np.isnan(x_train).any())\n",
    "x_t, y_t = build_timeseries(x_train, 3) # close price是第三个字段\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "print(\"Batch trimmed x_t size\",x_t.shape)\n",
    "print(\"Batch trimmed y_t size\",y_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建训练模型\n",
    "batch_input_shape中的第二个参数分别为None和TIME_STEPS的区别\n",
    "* None: 可接收任意长度\n",
    "* TIME_STEPS(60):长度规定为60\n",
    "\n",
    "@TODO 使用CuDNNLSTM提高执行速度\n",
    "\n",
    "@TODO 神经元的参数可配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm (CuDNNLSTM)       (20, 60, 100)             42800     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (20, 60, 100)             0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (20, 60)                  38880     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (20, 60)                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (20, 20)                  1220      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (20, 1)                   21        \n",
      "=================================================================\n",
      "Total params: 82,921\n",
      "Trainable params: 82,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 神经网络参数\n",
    "lstm_units = [100,60]\n",
    "lstm1_units = 100\n",
    "lstm2_units = 60\n",
    "dense1_units = 20\n",
    "dense2_units = 1 # 和输出数据的数量一致\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "  #rnn = tf.keras.layers.CuDNNGRU\n",
    "  # CuDNNLSTM的执行速度大概是普通的LSTM的5倍以上\n",
    "  lstm = tf.keras.layers.CuDNNLSTM\n",
    "else:\n",
    "  import functools\n",
    "  #rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "  lstm = functools.partial(tf.keras.layers.LSTM, recurrent_activation='sigmoid')\n",
    "\n",
    "def create_model():\n",
    "    lstm_model = tf.keras.Sequential()\n",
    "    # (batch_size, timesteps, data_dim)\n",
    "    # 没有激活函数？\n",
    "    lstm_model.add(lstm(lstm1_units,batch_input_shape=(BATCH_SIZE,TIME_STEPS,FEATURE_SIZE),\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True))\n",
    "    lstm_model.add(tf.keras.layers.Dropout(0.4))\n",
    "    lstm_model.add(lstm(lstm2_units))\n",
    "    lstm_model.add(tf.keras.layers.Dropout(0.4))\n",
    "    lstm_model.add(tf.keras.layers.Dense(dense1_units,activation='relu'))\n",
    "    # 这里只预测下一个价格，因此Dense的神经元个数为1\n",
    "    lstm_model.add(tf.keras.layers.Dense(dense2_units,activation='sigmoid'))\n",
    "    # 在这里SGD很难得到理想的结果，RMSprop一般可以比较好的收敛\n",
    "    #optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    optimizer = tf.train.AdamOptimizer();\n",
    "    #optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    #lstm_model.build()\n",
    "    return lstm_model\n",
    "\n",
    "\n",
    "model = None\n",
    "try:\n",
    "    model = pickle.load(open(\"lstm_model\", 'rb'))\n",
    "    print(\"Loaded saved model:\",model)\n",
    "    model.summary()\n",
    "except FileNotFoundError:\n",
    "    print(\"Model not found\")\n",
    "    model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备测试数据和验证数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_0 2752\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746dff3a1f844303ad4f5e7fbc666171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2752), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (2752, 60, 5) (2752,)\n",
      "Test size (1370, 60, 5) (1370,) (1370, 60, 5) (1370,)\n"
     ]
    }
   ],
   "source": [
    "x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)\n",
    "\n",
    "print(\"Test size\", x_test_t.shape, y_test_t.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH_SIZE对执行速度影响很大，以下是一些测试结果：\n",
    "\n",
    "|BATCH_SIZE|耗时（s/epoch)|\n",
    "|------|------|\n",
    "|10|50|\n",
    "|20|50|\n",
    "|512|5|\n",
    "\n",
    "但是，过大的batch_size会影响预测结果，参见：https://datascience.stackexchange.com/questions/16807/why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11180 samples, validate on 1360 samples\n",
      "Epoch 1/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 3.6078e-04\n",
      "Epoch 00001: val_loss improved from inf to 0.00174, saving model to ./outputs/2019-07-21/dropout_layers_0.4_0.4/best_model.h5\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 3.6113e-04 - val_loss: 0.0017\n",
      "Epoch 2/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.9054e-04\n",
      "Epoch 00002: val_loss improved from 0.00174 to 0.00139, saving model to ./outputs/2019-07-21/dropout_layers_0.4_0.4/best_model.h5\n",
      "11180/11180 [==============================] - 11s 996us/sample - loss: 2.9121e-04 - val_loss: 0.0014\n",
      "Epoch 3/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.7897e-04\n",
      "Epoch 00003: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 970us/sample - loss: 2.7976e-04 - val_loss: 0.0051\n",
      "Epoch 4/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.4934e-04\n",
      "Epoch 00004: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 12s 1ms/sample - loss: 2.4932e-04 - val_loss: 0.0048\n",
      "Epoch 5/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.5931e-04\n",
      "Epoch 00005: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 13s 1ms/sample - loss: 2.5944e-04 - val_loss: 0.0029\n",
      "Epoch 6/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.4009e-04\n",
      "Epoch 00006: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 13s 1ms/sample - loss: 2.4047e-04 - val_loss: 0.0022\n",
      "Epoch 7/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.3541e-04\n",
      "Epoch 00007: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 13s 1ms/sample - loss: 2.3523e-04 - val_loss: 0.0024\n",
      "Epoch 8/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.6579e-04\n",
      "Epoch 00008: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 13s 1ms/sample - loss: 2.6584e-04 - val_loss: 0.0024\n",
      "Epoch 9/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.8474e-04\n",
      "Epoch 00009: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 16s 1ms/sample - loss: 2.8481e-04 - val_loss: 0.0015\n",
      "Epoch 10/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 3.7096e-04\n",
      "Epoch 00010: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 14s 1ms/sample - loss: 3.7094e-04 - val_loss: 0.0032\n",
      "Epoch 11/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.8692e-04\n",
      "Epoch 00011: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 13s 1ms/sample - loss: 2.8667e-04 - val_loss: 0.0015\n",
      "Epoch 12/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.4049e-04\n",
      "Epoch 00012: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 16s 1ms/sample - loss: 2.3995e-04 - val_loss: 0.0066\n",
      "Epoch 13/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.9950e-04\n",
      "Epoch 00013: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.9943e-04 - val_loss: 0.0076\n",
      "Epoch 14/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.1713e-04\n",
      "Epoch 00014: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 990us/sample - loss: 2.1669e-04 - val_loss: 0.0130\n",
      "Epoch 15/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.5070e-04\n",
      "Epoch 00015: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 990us/sample - loss: 2.5042e-04 - val_loss: 0.0196\n",
      "Epoch 16/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.7871e-04\n",
      "Epoch 00016: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 2.7859e-04 - val_loss: 0.0018\n",
      "Epoch 17/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.9690e-04\n",
      "Epoch 00017: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 995us/sample - loss: 1.9674e-04 - val_loss: 0.0027\n",
      "Epoch 18/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.8679e-04\n",
      "Epoch 00018: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 994us/sample - loss: 1.8664e-04 - val_loss: 0.0048\n",
      "Epoch 19/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.9478e-04\n",
      "Epoch 00019: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 12s 1ms/sample - loss: 1.9471e-04 - val_loss: 0.0038\n",
      "Epoch 20/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.8424e-04\n",
      "Epoch 00020: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 12s 1ms/sample - loss: 1.8409e-04 - val_loss: 0.0060\n",
      "Epoch 21/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.0753e-04\n",
      "Epoch 00021: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 2.0774e-04 - val_loss: 0.0042\n",
      "Epoch 22/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.8989e-04\n",
      "Epoch 00022: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 995us/sample - loss: 1.8987e-04 - val_loss: 0.0023\n",
      "Epoch 23/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.9062e-04\n",
      "Epoch 00023: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.9059e-04 - val_loss: 0.0123\n",
      "Epoch 24/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.4198e-04\n",
      "Epoch 00024: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 991us/sample - loss: 2.4180e-04 - val_loss: 0.0118\n",
      "Epoch 25/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 3.2566e-04\n",
      "Epoch 00025: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 3.2517e-04 - val_loss: 0.0046\n",
      "Epoch 26/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.9760e-04\n",
      "Epoch 00026: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 950us/sample - loss: 1.9737e-04 - val_loss: 0.0019\n",
      "Epoch 27/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.7893e-04\n",
      "Epoch 00027: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 991us/sample - loss: 1.7889e-04 - val_loss: 0.0032\n",
      "Epoch 28/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.9190e-04\n",
      "Epoch 00028: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 992us/sample - loss: 1.9177e-04 - val_loss: 0.0040\n",
      "Epoch 29/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.7002e-04\n",
      "Epoch 00029: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 996us/sample - loss: 1.6977e-04 - val_loss: 0.0046\n",
      "Epoch 30/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.7165e-04\n",
      "Epoch 00030: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.7144e-04 - val_loss: 0.0038\n",
      "Epoch 31/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.6636e-04\n",
      "Epoch 00031: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 961us/sample - loss: 1.6628e-04 - val_loss: 0.0056\n",
      "Epoch 32/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.8160e-04\n",
      "Epoch 00032: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 995us/sample - loss: 1.8149e-04 - val_loss: 0.0015\n",
      "Epoch 33/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.5917e-04\n",
      "Epoch 00033: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 960us/sample - loss: 1.5901e-04 - val_loss: 0.0044\n",
      "Epoch 34/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.6183e-04\n",
      "Epoch 00034: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 997us/sample - loss: 1.6192e-04 - val_loss: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 1.5161e-04\n",
      "Epoch 00035: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 970us/sample - loss: 1.5153e-04 - val_loss: 0.0036\n",
      "Epoch 36/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.6030e-04\n",
      "Epoch 00036: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 965us/sample - loss: 1.6021e-04 - val_loss: 0.0046\n",
      "Epoch 37/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.6326e-04\n",
      "Epoch 00037: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 994us/sample - loss: 1.6339e-04 - val_loss: 0.0021\n",
      "Epoch 38/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.5052e-04\n",
      "Epoch 00038: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.5047e-04 - val_loss: 0.0033\n",
      "Epoch 39/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.6211e-04\n",
      "Epoch 00039: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 979us/sample - loss: 1.6248e-04 - val_loss: 0.0024\n",
      "Epoch 40/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.5295e-04\n",
      "Epoch 00040: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 973us/sample - loss: 1.5271e-04 - val_loss: 0.0042\n",
      "Epoch 41/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.3961e-04\n",
      "Epoch 00041: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 964us/sample - loss: 1.3943e-04 - val_loss: 0.0046\n",
      "Epoch 42/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.8672e-04\n",
      "Epoch 00042: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.8659e-04 - val_loss: 0.0017\n",
      "Epoch 43/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.4020e-04\n",
      "Epoch 00043: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.4021e-04 - val_loss: 0.0016\n",
      "Epoch 44/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.4193e-04\n",
      "Epoch 00044: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 971us/sample - loss: 1.4198e-04 - val_loss: 0.0021\n",
      "Epoch 45/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.5560e-04\n",
      "Epoch 00045: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 993us/sample - loss: 1.5532e-04 - val_loss: 0.0054\n",
      "Epoch 46/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.7535e-04\n",
      "Epoch 00046: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 994us/sample - loss: 1.7522e-04 - val_loss: 0.0071\n",
      "Epoch 47/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.5343e-04\n",
      "Epoch 00047: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 968us/sample - loss: 2.5367e-04 - val_loss: 0.0110\n",
      "Epoch 48/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 3.7738e-04\n",
      "Epoch 00048: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 989us/sample - loss: 3.7869e-04 - val_loss: 0.0066\n",
      "Epoch 49/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 3.0463e-04\n",
      "Epoch 00049: val_loss did not improve from 0.00139\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 3.0450e-04 - val_loss: 0.0019\n",
      "Epoch 50/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 3.3065e-04\n",
      "Epoch 00050: val_loss improved from 0.00139 to 0.00116, saving model to ./outputs/2019-07-21/dropout_layers_0.4_0.4/best_model.h5\n",
      "11180/11180 [==============================] - 11s 985us/sample - loss: 3.3072e-04 - val_loss: 0.0012\n",
      "Epoch 51/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.1311e-04\n",
      "Epoch 00051: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 996us/sample - loss: 2.1329e-04 - val_loss: 0.0015\n",
      "Epoch 52/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.7916e-04\n",
      "Epoch 00052: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 999us/sample - loss: 1.7912e-04 - val_loss: 0.0035\n",
      "Epoch 53/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.6415e-04\n",
      "Epoch 00053: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 973us/sample - loss: 2.6460e-04 - val_loss: 0.0064\n",
      "Epoch 54/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 4.1099e-04\n",
      "Epoch 00054: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 975us/sample - loss: 4.1142e-04 - val_loss: 0.0078\n",
      "Epoch 55/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 5.0130e-04\n",
      "Epoch 00055: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 964us/sample - loss: 5.0184e-04 - val_loss: 0.0075\n",
      "Epoch 56/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.6867e-04\n",
      "Epoch 00056: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 986us/sample - loss: 2.6944e-04 - val_loss: 0.0060\n",
      "Epoch 57/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 3.1362e-04\n",
      "Epoch 00057: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 986us/sample - loss: 3.1499e-04 - val_loss: 0.0058\n",
      "Epoch 58/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.8686e-04\n",
      "Epoch 00058: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 992us/sample - loss: 2.8712e-04 - val_loss: 0.0031\n",
      "Epoch 59/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.5235e-04\n",
      "Epoch 00059: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 998us/sample - loss: 2.5324e-04 - val_loss: 0.0027\n",
      "Epoch 60/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.6435e-04\n",
      "Epoch 00060: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 988us/sample - loss: 2.6567e-04 - val_loss: 0.0026\n",
      "Epoch 61/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.6656e-04\n",
      "Epoch 00061: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 968us/sample - loss: 2.6775e-04 - val_loss: 0.0024\n",
      "Epoch 62/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.9814e-04\n",
      "Epoch 00062: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 2.9958e-04 - val_loss: 0.0024\n",
      "Epoch 63/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.7676e-04\n",
      "Epoch 00063: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 991us/sample - loss: 2.7671e-04 - val_loss: 0.0016\n",
      "Epoch 64/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.4538e-04\n",
      "Epoch 00064: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 980us/sample - loss: 2.4532e-04 - val_loss: 0.0014\n",
      "Epoch 65/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.7068e-04\n",
      "Epoch 00065: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 999us/sample - loss: 2.7057e-04 - val_loss: 0.0016\n",
      "Epoch 66/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 3.1173e-04\n",
      "Epoch 00066: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 3.1150e-04 - val_loss: 0.0019\n",
      "Epoch 67/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.2317e-04\n",
      "Epoch 00067: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 991us/sample - loss: 2.2340e-04 - val_loss: 0.0018\n",
      "Epoch 68/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.9584e-04\n",
      "Epoch 00068: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 970us/sample - loss: 1.9575e-04 - val_loss: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.7926e-04\n",
      "Epoch 00069: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.7963e-04 - val_loss: 0.0020\n",
      "Epoch 70/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.7048e-04\n",
      "Epoch 00070: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.7033e-04 - val_loss: 0.0014\n",
      "Epoch 71/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.5201e-04\n",
      "Epoch 00071: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.5197e-04 - val_loss: 0.0016\n",
      "Epoch 72/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.6152e-04\n",
      "Epoch 00072: val_loss did not improve from 0.00116\n",
      "11180/11180 [==============================] - 11s 999us/sample - loss: 1.6158e-04 - val_loss: 0.0018\n",
      "Epoch 73/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.4686e-04\n",
      "Epoch 00073: val_loss improved from 0.00116 to 0.00114, saving model to ./outputs/2019-07-21/dropout_layers_0.4_0.4/best_model.h5\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.4681e-04 - val_loss: 0.0011\n",
      "Epoch 74/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.5145e-04\n",
      "Epoch 00074: val_loss did not improve from 0.00114\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.5130e-04 - val_loss: 0.0015\n",
      "Epoch 75/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.5737e-04\n",
      "Epoch 00075: val_loss improved from 0.00114 to 0.00088, saving model to ./outputs/2019-07-21/dropout_layers_0.4_0.4/best_model.h5\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.5747e-04 - val_loss: 8.7961e-04\n",
      "Epoch 76/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.5490e-04\n",
      "Epoch 00076: val_loss did not improve from 0.00088\n",
      "11180/11180 [==============================] - 12s 1ms/sample - loss: 1.5482e-04 - val_loss: 0.0024\n",
      "Epoch 77/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.5440e-04\n",
      "Epoch 00077: val_loss did not improve from 0.00088\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.5451e-04 - val_loss: 0.0013\n",
      "Epoch 78/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.6690e-04\n",
      "Epoch 00078: val_loss improved from 0.00088 to 0.00085, saving model to ./outputs/2019-07-21/dropout_layers_0.4_0.4/best_model.h5\n",
      "11180/11180 [==============================] - 11s 983us/sample - loss: 1.6675e-04 - val_loss: 8.4775e-04\n",
      "Epoch 79/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.7957e-04\n",
      "Epoch 00079: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 971us/sample - loss: 1.8014e-04 - val_loss: 0.0011\n",
      "Epoch 80/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.8670e-04\n",
      "Epoch 00080: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.8666e-04 - val_loss: 0.0023\n",
      "Epoch 81/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 2.1501e-04\n",
      "Epoch 00081: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 957us/sample - loss: 2.1577e-04 - val_loss: 9.7932e-04\n",
      "Epoch 82/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.1517e-04\n",
      "Epoch 00082: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 990us/sample - loss: 2.1509e-04 - val_loss: 0.0014\n",
      "Epoch 83/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.5798e-04\n",
      "Epoch 00083: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 977us/sample - loss: 2.5758e-04 - val_loss: 0.0014\n",
      "Epoch 84/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.6414e-04\n",
      "Epoch 00084: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 990us/sample - loss: 2.6409e-04 - val_loss: 0.0035\n",
      "Epoch 85/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 3.0175e-04\n",
      "Epoch 00085: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 978us/sample - loss: 3.0115e-04 - val_loss: 0.0021\n",
      "Epoch 86/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 3.2624e-04\n",
      "Epoch 00086: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 984us/sample - loss: 3.2580e-04 - val_loss: 0.0037\n",
      "Epoch 87/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 3.3568e-04\n",
      "Epoch 00087: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 998us/sample - loss: 3.3503e-04 - val_loss: 0.0033\n",
      "Epoch 88/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.7907e-04\n",
      "Epoch 00088: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 12s 1ms/sample - loss: 2.7902e-04 - val_loss: 0.0026\n",
      "Epoch 89/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.1246e-04\n",
      "Epoch 00089: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 12s 1ms/sample - loss: 2.1242e-04 - val_loss: 0.0040\n",
      "Epoch 90/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.9667e-04\n",
      "Epoch 00090: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 986us/sample - loss: 1.9639e-04 - val_loss: 0.0064\n",
      "Epoch 91/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.1613e-04\n",
      "Epoch 00091: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 971us/sample - loss: 2.1575e-04 - val_loss: 0.0082\n",
      "Epoch 92/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.2208e-04\n",
      "Epoch 00092: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 970us/sample - loss: 2.2166e-04 - val_loss: 0.0065\n",
      "Epoch 93/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 1.9770e-04\n",
      "Epoch 00093: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 960us/sample - loss: 1.9691e-04 - val_loss: 0.0076\n",
      "Epoch 94/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.7671e-04\n",
      "Epoch 00094: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 967us/sample - loss: 1.7699e-04 - val_loss: 0.0037\n",
      "Epoch 95/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.9117e-04\n",
      "Epoch 00095: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 961us/sample - loss: 1.9111e-04 - val_loss: 0.0037\n",
      "Epoch 96/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 2.1222e-04\n",
      "Epoch 00096: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 964us/sample - loss: 2.1262e-04 - val_loss: 0.0049\n",
      "Epoch 97/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 2.2133e-04\n",
      "Epoch 00097: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 972us/sample - loss: 2.2169e-04 - val_loss: 0.0053\n",
      "Epoch 98/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.3115e-04\n",
      "Epoch 00098: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 962us/sample - loss: 2.3114e-04 - val_loss: 0.0032\n",
      "Epoch 99/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 1.9045e-04\n",
      "Epoch 00099: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 962us/sample - loss: 1.9042e-04 - val_loss: 0.0042\n",
      "Epoch 100/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.4497e-04\n",
      "Epoch 00100: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 974us/sample - loss: 1.4517e-04 - val_loss: 0.0057\n",
      "Epoch 101/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.4380e-04\n",
      "Epoch 00101: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 971us/sample - loss: 1.4400e-04 - val_loss: 0.0063\n",
      "Epoch 102/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.3370e-04\n",
      "Epoch 00102: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.3378e-04 - val_loss: 0.0050\n",
      "Epoch 103/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.4604e-04\n",
      "Epoch 00103: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 996us/sample - loss: 1.4604e-04 - val_loss: 0.0056\n",
      "Epoch 104/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.7913e-04\n",
      "Epoch 00104: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.7918e-04 - val_loss: 0.0057\n",
      "Epoch 105/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.8650e-04\n",
      "Epoch 00105: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.8651e-04 - val_loss: 0.0057\n",
      "Epoch 106/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 2.0108e-04\n",
      "Epoch 00106: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 998us/sample - loss: 2.0092e-04 - val_loss: 0.0058\n",
      "Epoch 107/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 2.2932e-04\n",
      "Epoch 00107: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 2.2898e-04 - val_loss: 0.0060\n",
      "Epoch 108/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 2.0560e-04\n",
      "Epoch 00108: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 978us/sample - loss: 2.0548e-04 - val_loss: 0.0069\n",
      "Epoch 109/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.7405e-04\n",
      "Epoch 00109: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 988us/sample - loss: 1.7390e-04 - val_loss: 0.0045\n",
      "Epoch 110/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.9948e-04\n",
      "Epoch 00110: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 974us/sample - loss: 1.9929e-04 - val_loss: 0.0042\n",
      "Epoch 111/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.8985e-04\n",
      "Epoch 00111: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 974us/sample - loss: 1.8949e-04 - val_loss: 0.0059\n",
      "Epoch 112/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.4536e-04\n",
      "Epoch 00112: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 977us/sample - loss: 1.4540e-04 - val_loss: 0.0056\n",
      "Epoch 113/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.3632e-04\n",
      "Epoch 00113: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 991us/sample - loss: 1.3625e-04 - val_loss: 0.0056\n",
      "Epoch 114/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.3486e-04\n",
      "Epoch 00114: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 977us/sample - loss: 1.3478e-04 - val_loss: 0.0067\n",
      "Epoch 115/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.4220e-04\n",
      "Epoch 00115: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 984us/sample - loss: 1.4235e-04 - val_loss: 0.0078\n",
      "Epoch 116/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.3752e-04\n",
      "Epoch 00116: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 969us/sample - loss: 1.3751e-04 - val_loss: 0.0066\n",
      "Epoch 117/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.3723e-04\n",
      "Epoch 00117: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 984us/sample - loss: 1.3731e-04 - val_loss: 0.0067\n",
      "Epoch 118/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.4089e-04\n",
      "Epoch 00118: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 983us/sample - loss: 1.4092e-04 - val_loss: 0.0084\n",
      "Epoch 119/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.4039e-04\n",
      "Epoch 00119: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 992us/sample - loss: 1.4050e-04 - val_loss: 0.0066\n",
      "Epoch 120/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.4881e-04\n",
      "Epoch 00120: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 974us/sample - loss: 1.4886e-04 - val_loss: 0.0064\n",
      "Epoch 121/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.8646e-04\n",
      "Epoch 00121: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 985us/sample - loss: 1.8661e-04 - val_loss: 0.0063\n",
      "Epoch 122/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.7299e-04\n",
      "Epoch 00122: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 978us/sample - loss: 1.7280e-04 - val_loss: 0.0036\n",
      "Epoch 123/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.7196e-04\n",
      "Epoch 00123: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 979us/sample - loss: 1.7176e-04 - val_loss: 0.0036\n",
      "Epoch 124/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.4024e-04\n",
      "Epoch 00124: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 963us/sample - loss: 1.4013e-04 - val_loss: 0.0059\n",
      "Epoch 125/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 1.3997e-04\n",
      "Epoch 00125: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 985us/sample - loss: 1.4077e-04 - val_loss: 0.0072\n",
      "Epoch 126/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.3460e-04\n",
      "Epoch 00126: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 963us/sample - loss: 1.3516e-04 - val_loss: 0.0070\n",
      "Epoch 127/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.2501e-04\n",
      "Epoch 00127: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 979us/sample - loss: 1.2504e-04 - val_loss: 0.0061\n",
      "Epoch 128/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.1381e-04\n",
      "Epoch 00128: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 980us/sample - loss: 1.1380e-04 - val_loss: 0.0106\n",
      "Epoch 129/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.2918e-04\n",
      "Epoch 00129: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 977us/sample - loss: 1.2925e-04 - val_loss: 0.0062\n",
      "Epoch 130/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 1.1861e-04\n",
      "Epoch 00130: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 972us/sample - loss: 1.1886e-04 - val_loss: 0.0071\n",
      "Epoch 131/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 1.0787e-04\n",
      "Epoch 00131: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 969us/sample - loss: 1.0800e-04 - val_loss: 0.0077\n",
      "Epoch 132/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.1996e-04\n",
      "Epoch 00132: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 952us/sample - loss: 1.1992e-04 - val_loss: 0.0068\n",
      "Epoch 133/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.0863e-04\n",
      "Epoch 00133: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 962us/sample - loss: 1.0893e-04 - val_loss: 0.0056\n",
      "Epoch 134/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.1203e-04\n",
      "Epoch 00134: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 978us/sample - loss: 1.1197e-04 - val_loss: 0.0058\n",
      "Epoch 135/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.1009e-04\n",
      "Epoch 00135: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 10s 937us/sample - loss: 1.0985e-04 - val_loss: 0.0058\n",
      "Epoch 136/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.0980e-04\n",
      "Epoch 00136: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.0969e-04 - val_loss: 0.0055\n",
      "Epoch 137/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.1014e-04\n",
      "Epoch 00137: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 990us/sample - loss: 1.1038e-04 - val_loss: 0.0057\n",
      "Epoch 138/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.0331e-04\n",
      "Epoch 00138: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 992us/sample - loss: 1.0326e-04 - val_loss: 0.0071\n",
      "Epoch 139/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.0298e-04\n",
      "Epoch 00139: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.0313e-04 - val_loss: 0.0059\n",
      "Epoch 140/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.0818e-04\n",
      "Epoch 00140: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 986us/sample - loss: 1.0849e-04 - val_loss: 0.0076\n",
      "Epoch 141/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.0448e-04\n",
      "Epoch 00141: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 977us/sample - loss: 1.0447e-04 - val_loss: 0.0069\n",
      "Epoch 142/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.0942e-04\n",
      "Epoch 00142: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 987us/sample - loss: 1.0944e-04 - val_loss: 0.0065\n",
      "Epoch 143/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.1319e-04\n",
      "Epoch 00143: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 989us/sample - loss: 1.1320e-04 - val_loss: 0.0066\n",
      "Epoch 144/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 1.0467e-04\n",
      "Epoch 00144: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 996us/sample - loss: 1.0484e-04 - val_loss: 0.0061\n",
      "Epoch 145/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.0317e-04\n",
      "Epoch 00145: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 985us/sample - loss: 1.0311e-04 - val_loss: 0.0075\n",
      "Epoch 146/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.1033e-04\n",
      "Epoch 00146: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 996us/sample - loss: 1.1052e-04 - val_loss: 0.0062\n",
      "Epoch 147/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 9.9515e-05\n",
      "Epoch 00147: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 975us/sample - loss: 9.9543e-05 - val_loss: 0.0073\n",
      "Epoch 148/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.1376e-04\n",
      "Epoch 00148: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 985us/sample - loss: 1.1382e-04 - val_loss: 0.0066\n",
      "Epoch 149/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.0209e-04\n",
      "Epoch 00149: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 982us/sample - loss: 1.0194e-04 - val_loss: 0.0133\n",
      "Epoch 150/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.1528e-04\n",
      "Epoch 00150: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.1523e-04 - val_loss: 0.0186\n",
      "Epoch 151/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.1214e-04\n",
      "Epoch 00151: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 981us/sample - loss: 1.1206e-04 - val_loss: 0.0071\n",
      "Epoch 152/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.1024e-04\n",
      "Epoch 00152: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 978us/sample - loss: 1.1026e-04 - val_loss: 0.0070\n",
      "Epoch 153/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.2381e-04\n",
      "Epoch 00153: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 959us/sample - loss: 1.2373e-04 - val_loss: 0.0056\n",
      "Epoch 154/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.1348e-04\n",
      "Epoch 00154: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 986us/sample - loss: 1.1369e-04 - val_loss: 0.0058\n",
      "Epoch 155/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.0364e-04\n",
      "Epoch 00155: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 989us/sample - loss: 1.0362e-04 - val_loss: 0.0064\n",
      "Epoch 156/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.0923e-04\n",
      "Epoch 00156: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 972us/sample - loss: 1.0917e-04 - val_loss: 0.0058\n",
      "Epoch 157/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 9.6056e-05\n",
      "Epoch 00157: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 966us/sample - loss: 9.6042e-05 - val_loss: 0.0076\n",
      "Epoch 158/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 9.8444e-05\n",
      "Epoch 00158: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 987us/sample - loss: 9.8473e-05 - val_loss: 0.0070\n",
      "Epoch 159/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 1.0786e-04\n",
      "Epoch 00159: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 976us/sample - loss: 1.0780e-04 - val_loss: 0.0089\n",
      "Epoch 160/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.0701e-04\n",
      "Epoch 00160: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 974us/sample - loss: 1.0679e-04 - val_loss: 0.0076\n",
      "Epoch 161/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.0398e-04\n",
      "Epoch 00161: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 968us/sample - loss: 1.0391e-04 - val_loss: 0.0067\n",
      "Epoch 162/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.3299e-04\n",
      "Epoch 00162: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 985us/sample - loss: 1.3301e-04 - val_loss: 0.0060\n",
      "Epoch 163/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.0796e-04\n",
      "Epoch 00163: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 983us/sample - loss: 1.0794e-04 - val_loss: 0.0055\n",
      "Epoch 164/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.0069e-04\n",
      "Epoch 00164: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 971us/sample - loss: 1.0072e-04 - val_loss: 0.0066\n",
      "Epoch 165/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.1033e-04\n",
      "Epoch 00165: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 977us/sample - loss: 1.1028e-04 - val_loss: 0.0051\n",
      "Epoch 166/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.0261e-04\n",
      "Epoch 00166: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 981us/sample - loss: 1.0252e-04 - val_loss: 0.0070\n",
      "Epoch 167/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 1.0386e-04\n",
      "Epoch 00167: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 967us/sample - loss: 1.0383e-04 - val_loss: 0.0074\n",
      "Epoch 168/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 9.5406e-05\n",
      "Epoch 00168: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 978us/sample - loss: 9.5389e-05 - val_loss: 0.0071\n",
      "Epoch 169/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.0396e-04\n",
      "Epoch 00169: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 958us/sample - loss: 1.0389e-04 - val_loss: 0.0056\n",
      "Epoch 170/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11140/11180 [============================>.] - ETA: 0s - loss: 9.3614e-05\n",
      "Epoch 00170: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 9.3731e-05 - val_loss: 0.0086\n",
      "Epoch 171/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.0368e-04\n",
      "Epoch 00171: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1000us/sample - loss: 1.0353e-04 - val_loss: 0.0052\n",
      "Epoch 172/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 9.5074e-05\n",
      "Epoch 00172: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 988us/sample - loss: 9.5257e-05 - val_loss: 0.0070\n",
      "Epoch 173/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 9.2340e-05\n",
      "Epoch 00173: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 996us/sample - loss: 9.2424e-05 - val_loss: 0.0062\n",
      "Epoch 174/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 9.2891e-05\n",
      "Epoch 00174: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 988us/sample - loss: 9.2786e-05 - val_loss: 0.0056\n",
      "Epoch 175/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 9.3676e-05\n",
      "Epoch 00175: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 987us/sample - loss: 9.3710e-05 - val_loss: 0.0057\n",
      "Epoch 176/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 8.7433e-05\n",
      "Epoch 00176: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 980us/sample - loss: 8.7394e-05 - val_loss: 0.0056\n",
      "Epoch 177/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 8.7808e-05\n",
      "Epoch 00177: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 975us/sample - loss: 8.7723e-05 - val_loss: 0.0066\n",
      "Epoch 178/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 9.2036e-05\n",
      "Epoch 00178: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 997us/sample - loss: 9.2062e-05 - val_loss: 0.0053\n",
      "Epoch 179/300\n",
      "11100/11180 [============================>.] - ETA: 0s - loss: 9.1082e-05\n",
      "Epoch 00179: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 990us/sample - loss: 9.0920e-05 - val_loss: 0.0066\n",
      "Epoch 180/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 1.0308e-04\n",
      "Epoch 00180: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 982us/sample - loss: 1.0305e-04 - val_loss: 0.0090\n",
      "Epoch 181/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.0000e-04\n",
      "Epoch 00181: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 946us/sample - loss: 9.9940e-05 - val_loss: 0.0115\n",
      "Epoch 182/300\n",
      "11160/11180 [============================>.] - ETA: 0s - loss: 1.1377e-04\n",
      "Epoch 00182: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 12s 1ms/sample - loss: 1.1366e-04 - val_loss: 0.0136\n",
      "Epoch 183/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.2275e-04\n",
      "Epoch 00183: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.2237e-04 - val_loss: 0.0177\n",
      "Epoch 184/300\n",
      "11120/11180 [============================>.] - ETA: 0s - loss: 1.1947e-04\n",
      "Epoch 00184: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 1.1923e-04 - val_loss: 0.0084\n",
      "Epoch 185/300\n",
      "11140/11180 [============================>.] - ETA: 0s - loss: 9.9318e-05\n",
      "Epoch 00185: val_loss did not improve from 0.00085\n",
      "11180/11180 [==============================] - 11s 1ms/sample - loss: 9.9247e-05 - val_loss: 0.0085\n",
      "Epoch 186/300\n",
      " 3460/11180 [========>.....................] - ETA: 7s - loss: 1.4230e-05"
     ]
    }
   ],
   "source": [
    "is_update_model = True\n",
    "if model is None or is_update_model:\n",
    "#     from keras import backend as K\n",
    "#     print(\"Building model...\")\n",
    "#     print(\"checking if GPU available\", K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='min', verbose=1,\n",
    "                       patience=40, min_delta=0.0001)\n",
    "    \n",
    "    mcp = ModelCheckpoint(os.path.join(OUTPUT_PATH,\n",
    "                          \"best_model.h5\"), monitor='val_loss', verbose=1,\n",
    "                          save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "\n",
    "    # Not used here. But leaving it here as a reminder for future\n",
    "    r_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=30, \n",
    "                                  verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    \n",
    "    csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'training_log_' + time.ctime().replace(\" \",\"_\") + '.log'), append=True)\n",
    "    \n",
    "    # earlyStopping无法执行？\n",
    "    history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=1, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                        trim_dataset(y_val, BATCH_SIZE)),callbacks=[es,mcp,csv_logger])\n",
    "    \n",
    "    print(\"saving model...\")\n",
    "    pickle.dump(model, open(\"lstm_model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(x_test_t, y_test_t, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测\n",
    "根据x_test_t进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the training data\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'train_vis_BS_'+str(BATCH_SIZE)+\"_\"+time.ctime()+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred(pred, real, title):\n",
    "    \"\"\"绘制预测和实际的比较图\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(pred)\n",
    "    plt.plot(real)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Price')\n",
    "    plt.xlabel('Days')\n",
    "    plt.legend(['Prediction', 'Real'])\n",
    "    plt.show()\n",
    "\n",
    "def stock_pred(x, y, debug=True, title='Prediction vs Real Stock Price', batch_size=BATCH_SIZE):\n",
    "    '''\n",
    "    收盘价预测\n",
    "    但是，这里没有越雷池一步，即没有往前走哪怕一小步。\n",
    "    x: 输入样本列表\n",
    "    y：labels，真实股价列表\n",
    "    batch_size: 进行预测时的batch_size。当给出的样本数量小时，需要减少batch_size以适配之\n",
    "    '''\n",
    "    print('batch size:',batch_size)\n",
    "    y_pred = model.predict(trim_dataset(x, batch_size), batch_size=batch_size)\n",
    "    if debug==True:\n",
    "        print(y_pred)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_test_t = trim_dataset(y, batch_size)\n",
    "    error = mean_squared_error(y_test_t, y_pred)\n",
    "    if debug==True:\n",
    "        print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "        print(y_pred[0:15])\n",
    "        print(y_test_t[0:15])\n",
    "    y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_pred)\n",
    "    y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_t)\n",
    "    if debug==True:\n",
    "        print(y_pred_org[0:15])\n",
    "        print(y_test_t_org[0:15])\n",
    "\n",
    "    # Visualize the prediction\n",
    "    plot_pred(y_pred_org, y_test_t_org, title)\n",
    "    plt.savefig(os.path.join(OUTPUT_PATH, 'pred_vs_real_BS'+str(batch_size)+\"_\"+str(len(x))+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_pred(x_test_t, y_test_t)\n",
    "print_time(\"program completed \", stime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最近N天的走势放大图\n",
    "将测试集的最近N天的走势放大了来看看，似乎预测曲线更加平顺。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for days in (200,100,60,20,10):\n",
    "#     y_pred = y_pred_org[-days:]\n",
    "#     y_test_t = y_test_t_org[-days:]\n",
    "#     plot_pred(y_pred,y_test_t,'Prediction vs Real Stock Price(latest ' + str(days) + ' days)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最近N天的走势预测\n",
    "在执行这个block之前，最好将测试数据重新生成一遍，以免相互影响造成测试数据的改变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在单步预测之前，需要根据checkpoint重构模型，输入的shape修正为(1,1)，否则输入必须和训练时的batch_size相同，很受限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "_, x_test_t = np.split(x_temp,2)\n",
    "_, y_test_t = np.split(y_temp,2)\n",
    "\n",
    "for days in (200,100,60,20):\n",
    "    x = x_test_t[-days:]\n",
    "    y = y_test_t[-days:]\n",
    "    stock_pred(x, y, debug=False, title='Prediction vs Real Stock Price(latest ' + str(days) + ' days)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后记\n",
    "\n",
    "* 如何逐步的观察预测的结果？比如给出前60天的数据作为x_test，然后只预测出下一天的收盘价？\n",
    "* 如果预测是开盘价呢？\n",
    "* 改造为猜第二天的涨跌\n",
    "* 改造成many-to-many的案例，即根据前N天的数据预测后M天的收盘价\n",
    "* 如何显示真实的日期？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
