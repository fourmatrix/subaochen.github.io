{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#最近N天的走势放大图\" data-toc-modified-id=\"最近N天的走势放大图-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>最近N天的走势放大图</a></span></li><li><span><a href=\"#最近N天的走势预测\" data-toc-modified-id=\"最近N天的走势预测-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>最近N天的走势预测</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RNN预测股票下一日的收盘价\n",
    "\n",
    "这基本是一个不可能完成的任务，不过作为RNN的练习，还是一个不错的题目：有数据，有场景，有吸引力。本案例主要参考：https://github.com/DarkKnight1991/Stock-Price-Prediction ，这是一个`many-to-one`的RNN案例，即通过前60日股价数据（open,close,high,low,volume）预测下一日的收盘价，其中feature size=5。\n",
    "\n",
    "先上运行结果图：![prediction result](https://raw.githubusercontent.com/subaochen/subaochen.github.io/master/images/stock-pred-result.png)\n",
    "\n",
    "But，这里仅仅是一个练习，不作为投资参考，请参见：https://www.blueskycapitalmanagement.com/machine-learning-in-finance-why-you-should-not-use-lstms-to-predict-the-stock-market/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入必要的包\n",
    "这里使用了sklearn包的MinMaxScaler进行数据的预处理，主要是归一化处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import pickle\n",
    "#from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "#from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import optimizers\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "os.environ['TZ'] = 'Asia/Shanghai'  # to set timezone; needed when running on cloud\n",
    "time.tzset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集中设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 20,  # 20<16<10, 25 was a bust\n",
    "    \"epochs\": 300, # 由于启用了earlyStopping机制，通常会提前终止\n",
    "    \"lr\": 0.00010000, # 学习率\n",
    "    \"time_steps\": 60 # RNN的(最大？)滑动窗口大小，这是使用60日的数据预测下一日的某个特征\n",
    "}\n",
    "\n",
    "iter_changes = \"dropout_layers_0.4_0.4\"\n",
    "DATA_FILE=\"ge.us.txt\"\n",
    "PATH_TO_DRIVE_ML_DATA=\"./\"\n",
    "INPUT_PATH = PATH_TO_DRIVE_ML_DATA+\"inputs\"\n",
    "OUTPUT_PATH = PATH_TO_DRIVE_ML_DATA+\"outputs/\"+time.strftime(\"%Y-%m-%d\")+\"/\"+iter_changes\n",
    "TIME_STEPS = params[\"time_steps\"]\n",
    "BATCH_SIZE = params[\"batch_size\"]\n",
    "stime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory recreated ./outputs/2019-07-20/dropout_layers_0.4_0.4\n"
     ]
    }
   ],
   "source": [
    "# check if directory already exists\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    print(\"Directory created\", OUTPUT_PATH)\n",
    "else:\n",
    "    os.rename(OUTPUT_PATH, OUTPUT_PATH+str(stime))\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    print(\"Directory recreated\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造训练数据的方法参见下图，注意颜色相同的矩形块框出了输入数据和预测数据，图中的窗口尺寸（time_steps）是3，即使用前3天的数据预测下一日的收盘价。显然，如果样本数为`N`，则可划分的输入样本数为`N-time_steps`。\n",
    "\n",
    "![build serial time data](https://raw.githubusercontent.com/subaochen/subaochen.github.io/master/images/stock-pred-data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ge.us.txt']\n"
     ]
    }
   ],
   "source": [
    "def print_time(text, stime):\n",
    "    seconds = (time.time()-stime)\n",
    "    print(text, seconds//60,\"minutes : \",np.round(seconds%60),\"seconds\")\n",
    "\n",
    "\n",
    "def trim_dataset(mat,batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if no_of_rows_drop > 0:\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat\n",
    "\n",
    "\n",
    "def build_timeseries(mat, y_col_index):\n",
    "    \"\"\"\n",
    "    Converts ndarray into timeseries format and supervised data format. Takes first TIME_STEPS\n",
    "    number of rows as input and sets the TIME_STEPS+1th data as corresponding output and so on.\n",
    "    :param mat: ndarray which holds the dataset，shape：\n",
    "    :param y_col_index: index of column which acts as output\n",
    "    :return: returns two ndarrays-- input and output in format suitable to feed\n",
    "    to LSTM.\n",
    "    \"\"\"\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    # 先构造空的x，y\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    print(\"dim_0\",dim_0)\n",
    "    for i in tqdm_notebook(range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i] # [i,TIME_STEPS+i)，x不包含TIME_STEPS+i这一行\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index] # TIME_STEPS行x对应一个y。如果存在多个预测，修改这里\n",
    "#         if i < 10:\n",
    "#           print(i,\"-->\", x[i,-1,:], y[i])\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "stime = time.time()\n",
    "print(os.listdir(INPUT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造训练数据\n",
    "\n",
    "Again，RNN的输入数据要求的shape是(batch_size, time_steps, feature_size)\n",
    "\n",
    "数据归一化处理的公式：$$X_{scaled}=\\frac{X-X_{min}(axis=0)}{X_{max}(axis=0)-X_{min}(axis=0)}\\times(max-min)+min$$\n",
    "\n",
    "使用MinMaxScaler进行归一化处理时要注意，在训练数据集和测试数据集上要执行同样的缩放标准，因此fit只能执行一次，或者只能执行一次fit_transform函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14058, 7)\n",
      "             Date   Open    High    Low  Close     Volume  OpenInt\n",
      "14053  2017-11-06  20.52  20.530  20.08  20.13   60641787        0\n",
      "14054  2017-11-07  20.17  20.250  20.12  20.21   41622851        0\n",
      "14055  2017-11-08  20.21  20.320  20.07  20.12   39672190        0\n",
      "14056  2017-11-09  20.04  20.071  19.85  19.99   50831779        0\n",
      "14057  2017-11-10  19.98  20.680  19.90  20.49  100698474        0\n",
      "Date        object\n",
      "Open       float64\n",
      "High       float64\n",
      "Low        float64\n",
      "Close      float64\n",
      "Volume       int64\n",
      "OpenInt      int64\n",
      "dtype: object\n",
      "Train--Test size 11246 2812\n",
      "Deleting unused dataframes of total size(KB) 3267\n",
      "Are any NaNs present in train/test matrices? False False\n",
      "dim_0 11186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de390650049e426fae1b99856ec5ed43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11186), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of time-series i/o (11186, 60, 5) (11186,)\n",
      "Batch trimmed x_t size (11180, 60, 5)\n",
      "Batch trimmed y_t size (11180,)\n"
     ]
    }
   ],
   "source": [
    "df_ge = pd.read_csv(os.path.join(INPUT_PATH, DATA_FILE))\n",
    "print(df_ge.shape)\n",
    "print(df_ge.tail())\n",
    "tqdm_notebook.pandas('Processing...')\n",
    "print(df_ge.dtypes)\n",
    "# 训练需要的数据字段列表，去掉了日期和OpenInt\n",
    "train_cols = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n",
    "df_train, df_test = train_test_split(df_ge, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "print(\"Train--Test size\", len(df_train), len(df_test))\n",
    "\n",
    "# scale the feature MinMax, build array\n",
    "# 只需要感兴趣的字段\n",
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "# 几个方法的区别，参见：https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "# fit，计算给定集合的最大值和最小值，以便归一化计算\n",
    "# transform，在fit的基础上对数据进行归一化处理\n",
    "# fit_transform，将fit和transform结合起来，在一个函数完成\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "# 注意这里不能再次fit，在训练和测试数据集上要执行同样的缩放标准\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols].values)\n",
    "\n",
    "print(\"Deleting unused dataframes of total size(KB)\",\n",
    "      (sys.getsizeof(df_ge)+sys.getsizeof(df_train)+sys.getsizeof(df_test))//1024)\n",
    "\n",
    "del df_ge\n",
    "del df_test\n",
    "del df_train\n",
    "del x\n",
    "\n",
    "print(\"Are any NaNs present in train/test matrices?\",np.isnan(x_train).any(), np.isnan(x_train).any())\n",
    "x_t, y_t = build_timeseries(x_train, 3) # close price是第三个字段\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "print(\"Batch trimmed x_t size\",x_t.shape)\n",
    "print(\"Batch trimmed y_t size\",y_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建训练模型\n",
    "batch_input_shape中的第二个参数分别为None和TIME_STEPS的区别\n",
    "* None: 可接收任意长度\n",
    "* TIME_STEPS(60):长度规定为60\n",
    "\n",
    "@TODO 使用CuDNNLSTM提高执行速度\n",
    "\n",
    "@TODO 神经元的参数可配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer cu_dnnlstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 20, 60, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f94ffb2aeb4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lstm_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded saved model:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lstm_model'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f94ffb2aeb4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f94ffb2aeb4f>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mrecurrent_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'glorot_uniform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         stateful=True))\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm2_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m           \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m           \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m     input_spec.assert_input_compatibility(\n\u001b[0;32m-> 1591\u001b[0;31m         self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   1592\u001b[0m     \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer cu_dnnlstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 20, 60, 5]"
     ]
    }
   ],
   "source": [
    "# 神经网络参数\n",
    "lstm1_units = 100\n",
    "lstm2_units = 60\n",
    "dense1_units = 20\n",
    "dense2_units = 1 # 和输出数据的数量一致\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "  #rnn = tf.keras.layers.CuDNNGRU\n",
    "  # CuDNNLSTM的执行速度大概是普通的LSTM的5倍以上\n",
    "  lstm = tf.keras.layers.CuDNNLSTM\n",
    "else:\n",
    "  import functools\n",
    "  #rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "  lstm = functools.partial(tf.keras.layers.LSTM, recurrent_activation='sigmoid')\n",
    "\n",
    "def create_model():\n",
    "    lstm_model = tf.keras.Sequential()\n",
    "    # (batch_size, timesteps, data_dim)\n",
    "    # 没有激活函数？\n",
    "    lstm_model.add(lstm(lstm1_units,input_shape=(BATCH_SIZE,TIME_STEPS,x_t.shape[2]),\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True))\n",
    "    lstm_model.add(tf.keras.layers.Dropout(0.4))\n",
    "    lstm_model.add(lstm(lstm2_units))\n",
    "    lstm_model.add(tf.keras.layers.Dropout(0.4))\n",
    "    lstm_model.add(tf.keras.layers.Dense(dense1_units,activation='relu'))\n",
    "    # 这里只预测下一个价格，因此Dense的神经元个数为1\n",
    "    lstm_model.add(tf.keras.layers.Dense(dense2_units,activation='sigmoid'))\n",
    "    # 在这里SGD很难得到理想的结果，RMSprop一般可以比较好的收敛\n",
    "    #optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    optimizer = tf.train.AdamOptimizer();\n",
    "    #optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    #lstm_model.build()\n",
    "    return lstm_model\n",
    "\n",
    "\n",
    "model = None\n",
    "try:\n",
    "    model = pickle.load(open(\"lstm_model\", 'rb'))\n",
    "    print(\"Loaded saved model:\",model)\n",
    "    model.summary()\n",
    "except FileNotFoundError:\n",
    "    print(\"Model not found\")\n",
    "    model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备测试数据和验证数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)\n",
    "\n",
    "print(\"Test size\", x_test_t.shape, y_test_t.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH_SIZE对执行速度影响很大，以下是一些测试结果：\n",
    "\n",
    "|BATCH_SIZE|耗时（s/epoch)|\n",
    "|------|------|\n",
    "|10|50|\n",
    "|20|50|\n",
    "|512|5|\n",
    "\n",
    "但是，过大的batch_size会影响预测结果，参见：https://datascience.stackexchange.com/questions/16807/why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "is_update_model = True\n",
    "if model is None or is_update_model:\n",
    "#     from keras import backend as K\n",
    "#     print(\"Building model...\")\n",
    "#     print(\"checking if GPU available\", K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=40, min_delta=0.0001)\n",
    "    \n",
    "    mcp = ModelCheckpoint(os.path.join(OUTPUT_PATH,\n",
    "                          \"best_model.h5\"), monitor='val_loss', verbose=1,\n",
    "                          save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "\n",
    "    # Not used here. But leaving it here as a reminder for future\n",
    "    r_lr_plat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=30, \n",
    "                                  verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    \n",
    "    csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'training_log_' + time.ctime().replace(\" \",\"_\") + '.log'), append=True)\n",
    "    \n",
    "    history = model.fit(x_t, y_t, epochs=params[\"epochs\"], verbose=1, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                        trim_dataset(y_val, BATCH_SIZE)), callbacks=[es, csv_logger, mcp])\n",
    "    \n",
    "    print(\"saving model...\")\n",
    "    pickle.dump(model, open(\"lstm_model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.evaluate(x_test_t, y_test_t, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测\n",
    "根据x_test_t进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the training data\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'train_vis_BS_'+str(BATCH_SIZE)+\"_\"+time.ctime()+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred(pred, real, title):\n",
    "    \"\"\"绘制预测和实际的比较图\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(pred)\n",
    "    plt.plot(real)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Price')\n",
    "    plt.xlabel('Days')\n",
    "    plt.legend(['Prediction', 'Real'])\n",
    "    plt.show()\n",
    "\n",
    "def stock_pred(x, y, debug=True, title='Prediction vs Real Stock Price', batch_size=BATCH_SIZE):\n",
    "    '''\n",
    "    收盘价预测\n",
    "    但是，这里没有越雷池一步，即没有往前走哪怕一小步。\n",
    "    x: 输入样本列表\n",
    "    y：labels，真实股价列表\n",
    "    batch_size: 进行预测时的batch_size。当给出的样本数量小时，需要减少batch_size以适配之\n",
    "    '''\n",
    "    print('batch size:',batch_size)\n",
    "    y_pred = model.predict(trim_dataset(x, batch_size), batch_size=batch_size)\n",
    "    if debug==True:\n",
    "        print(y_pred)\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_test_t = trim_dataset(y, batch_size)\n",
    "    error = mean_squared_error(y_test_t, y_pred)\n",
    "    if debug==True:\n",
    "        print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "        print(y_pred[0:15])\n",
    "        print(y_test_t[0:15])\n",
    "    y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_pred)\n",
    "    y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_t)\n",
    "    if debug==True:\n",
    "        print(y_pred_org[0:15])\n",
    "        print(y_test_t_org[0:15])\n",
    "\n",
    "    # Visualize the prediction\n",
    "    plot_pred(y_pred_org, y_test_t_org, title)\n",
    "    plt.savefig(os.path.join(OUTPUT_PATH, 'pred_vs_real_BS'+str(batch_size)+\"_\"+str(len(x))+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_pred(x_test_t, y_test_t)\n",
    "print_time(\"program completed \", stime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最近N天的走势放大图\n",
    "将测试集的最近N天的走势放大了来看看，似乎预测曲线更加平顺。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for days in (200,100,60,20,10):\n",
    "#     y_pred = y_pred_org[-days:]\n",
    "#     y_test_t = y_test_t_org[-days:]\n",
    "#     plot_pred(y_pred,y_test_t,'Prediction vs Real Stock Price(latest ' + str(days) + ' days)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最近N天的走势预测\n",
    "在执行这个block之前，最好将测试数据重新生成一遍，以免相互影响造成测试数据的改变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在单步预测之前，需要根据checkpoint重构模型，输入的shape修正为(1,1)，否则输入必须和训练时的batch_size相同，很受限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "_, x_test_t = np.split(x_temp,2)\n",
    "_, y_test_t = np.split(y_temp,2)\n",
    "\n",
    "for days in (200,100,60,20):\n",
    "    x = x_test_t[-days:]\n",
    "    y = y_test_t[-days:]\n",
    "    stock_pred(x, y, debug=False, title='Prediction vs Real Stock Price(latest ' + str(days) + ' days)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 后记\n",
    "\n",
    "* 如何逐步的观察预测的结果？比如给出前60天的数据作为x_test，然后只预测出下一天的收盘价？\n",
    "* 如果预测是开盘价呢？\n",
    "* 改造为猜第二天的涨跌\n",
    "* 改造成many-to-many的案例，即根据前N天的数据预测后M天的收盘价\n",
    "* 如何显示真实的日期？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
